<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/programming/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2018-12-20T09:21:38+00:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Query Object Pattern and Entity Framework - Making Readable Queries]]></title>
    <link href="http://rahulpnath.com/blog/query-object-pattern-and-entity-framework-making-readable-queries/"/>
    <updated>2018-12-20T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/query-object-pattern-and-entity-framework-making-readable-queries</id>
    <content type="html"><![CDATA[<p>Search is a common requirement for most of the applications that we build today. Searching for data often includes multiple fields, data types, and data from multiple tables (especially when using a relational database). I was recently building a Search page which involved searching for Orders - users needed the ability to search by different criteria such as the employee who created the order, orders for a customer, orders between particular dates, order status, an address of delivery. Order criteria are optional, and they allow to narrow down on your search with additional parameters. We were building an API endpoint to query this data based on the parameters using EF Core backed by Azure SQL.</p>

<p>In this post, we go through the code iterations that I made to improve on the readability of the query and keep it contained in a single place. The intention is to create a Query Object like structure that contains all query logic and keep it centralized and readable.</p>

<blockquote><p><em>A <a href="https://martinfowler.com/eaaCatalog/queryObject.html">Query Object</a> is an interpreter [Gang of Four], that is, a structure of objects that can form itself into a SQL query. You can create this query by referring to classes and fields rather than tables and columns. In this way, those who write the queries can do so independently of the database schema, and changes to the schema can be localized in a single place.</em></p></blockquote>

<pre><code class="csharp Query Object capturing the Search Criteria">public class OrderSummaryQuery
{
    public int? CustomerId { get; set; }
    public DateRange DateRange { get; set; }
    public string Employee { get; set; }
    public string Address { get; set;}
    public OrderStatus OrderStatus { get; set; }
}
</code></pre>

<p>I have removed the final projection in all the queries below to keep the code to a minimum. We will go through all the iterations to make the code more readable, keeping the generated SQL query efficient as possible.</p>

<h3>Iteration 1 - Crude Form</h3>

<p>Let&rsquo;s start with the crudest form of the query stating all possible combinations of the query. Since all properties are nullable, check if a value exists before using it in the query.</p>

<pre><code class="csharp">(from order in _context.Order
join od in _context.OrderDelivery on order.Id equals od.OrderId
join customer in _context.Customer on order.CustomerId equals customer.Id
where order.Status == OrderStatus.Quote &amp;&amp;
      order.Active == true &amp;&amp;
      (query.Employee == null || 
      (order.CreatedBy == query.Employee || customer.Employee == query.Employee)) &amp;&amp;
      (!query.CustomerId.HasValue ||
      customer.Id == query.CustomerId.Value) &amp;&amp;
      (query.DateRange == null || 
      order.Created &gt;= query.DateRange.StartDate &amp;&amp; order.Created &lt;= query.DateRange.EndDate))
</code></pre>

<h3>Iteration 2 - Separating into Multiple Lines</h3>

<p>With all those explicit AND (&amp;&amp;) clauses the query is hard to understand and keep up. Splitting them into multiple where clauses make it more cleaner and keep each search criteria independent. The end SQL query that gets generated remains the same in this case.</p>

<blockquote><p><a href="https://rahulpnath.com/blog/left-align-your-code-for-better-readability/">Aesthetics of code</a> is as important as the code you write. Aligning is an important part that contributes to the overall aesthetics of code.</p></blockquote>

<pre><code class="csharp">from order in _context.Order
join od in _context.OrderDelivery on order.Id equals od.OrderId
join customer in _context.Customer on order.CustomerId equals customer.Id
where order.Status == orderStatus &amp;&amp; order.Active == true
where query.Employee == null ||
      order.CreatedBy == query.Employee || customer.Employee == query.Employee
where !query.CustomerId.HasValue || customer.Id == query.CustomerId.Value
where query.DateRange == null ||
      (order.Created &gt;= query.DateRange.StartDate &amp;&amp; order.Created &lt;= query.DateRange.EndDate)
</code></pre>

<h3>Iteration 3 - Refactor to Expressions</h3>

<p>Now that each criterion is independently visible let&rsquo;s make each of the <em>where</em> clause more readable. Refactoring them into C# class functions makes the generated SQL inefficient, as EF cannot transform C# functions into SQL.  Such conditions in a standard C# function gets evaluated on the client site, after retrieving all data from the server. Depending on the size of your data, this is something you need <a href="https://docs.microsoft.com/en-us/ef/core/querying/client-eval#client-evaluation-performance-issues">to be aware of</a>.</p>

<p>However, if you use <a href="https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/ef/language-reference/expressions-in-linq-to-entities-queries">Expressions</a> those get transformed to evaluate on the server. Since all of the conditions on our where clauses can be represented as an Expression, let&rsquo;s move those to the Query object class as properties returning Expressions. Since we need data from multiple tables, the intermediate projection <em>OrderSummaryQueryResult</em> helps to work with data from the multiple tables. All our expressions take the <em>OrderSummaryQueryResult</em> projection and perform the appropriate conditions on them.</p>

<pre><code class="csharp   ">public class OrderSummaryQuery
{
    public Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; BelongsToUser
    {
        get
        {
            return (a) =&gt; Employee == null ||
                      a.Order.CreatedBy == Employee || a.Customer.Employee == Employee;
        }
    }

    public Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; IsActiveOrder...
    public Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; ForCustomer...
    public Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; InDateRange...
}
</code></pre>

<pre><code class="csharp Refactored to use Expressions ">(from order in _context.Order
 join od in _context.OrderDelivery on order.Id equals od.OrderId
 join customer in _context.Customer on order.CustomerId equals customer.Id
 select new OrderSummaryQueryResult() 
    { Customer = customer, Order =    order, OrderDelivery = od })
.Where(query.IsActiveOrder)
.Where(query.BelongsToUser)
.Where(query.ForCustomer)
.Where(query.InDateRange)
</code></pre>

<pre><code class="sql Generated SQL when order status and employee name is set">SELECT [customer].[Name] AS [Customer], [order].[OrderNumber] AS [Number],
       [od].[Address], [order].[Created] AS [CreatedDate]
FROM [Order] AS [order]
INNER JOIN [OrderDelivery] AS [od] ON [order].[Id] = [od].[OrderId]
INNER JOIN [Customer] AS [customer] ON [order].[CustomerId] = [customer].[Id]
WHERE (([order].[Active] = 1) AND ([order].[Status] = @__OrderStatus_0)) AND 
      (([order].[CreatedBy] = @__employee_1) OR ([customer].[Employee] = @__employee_2))
</code></pre>

<div class="alert alert-warning">
If you use constructor initialization for intermediate projection, *OrderSummaryQueryResult* the where clauses gets executed on the client side. So use the object initializer syntax to create the intermediate projection.
</div>


<h3>Iteration 4 - Refactoring to Extension method</h3>

<p>After the last iteration, we have a query that is easy to read and understand. We also have all queries consolidated within the query object, and it acts as a one place holding all the queries. However, something still felt not right, and I had a quick chat with my friend <a href="https://twitter.com/zpbappi">Bappi</a>, and we refined it further. The above query has too many where clauses and it was just repeating for each of the filters. To encapsulate this further, I moved all the filter expressions to be returned as an Enumerable and wrote an extension method, <em>ApplyAllFilters</em>, to execute them all.</p>

<pre><code class="csharp Expose one property for all the filters   ">public class OrderSummaryQuery
{
    public IEnumerable&lt;Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt;&gt; AllFilters
    {
        get
        {
            yield return IsActiveOrderStatus;
            yield return BelongsToUser;
            yield return BelongsToCustomer;
            yield return FallsInDateRange;
        }
    }

    private Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; BelongsToUser...
    private Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; IsActiveOrder...
    private Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; ForCustomer...
    private Expression&lt;Func&lt;OrderSummaryQueryResult, bool&gt;&gt; InDateRange...
}

... 

// Extension Method on IQueryable
{
    public static IQueryable&lt;T&gt; ApplyAllFilters&lt;T&gt;(
        this IQueryable&lt;T&gt; queryable,
        IEnumerable&lt;Expression&lt;Func&lt;T, bool&gt;&gt;&gt; filters)
    {
        foreach (var filter in filters)
            queryable = queryable.Where(filter);

        return queryable;
    }
}
</code></pre>

<pre><code class="csharp">{
    (from order in _context.Order
    join od in orderDeliveries on order.Id equals od.OrderId
    join customer in _context.Customer on order.CustomerId equals customer.Id
    select new OrderSummaryQueryResult() { Customer = customer, Order = order, OrderDelivery = od })
    .ApplyAllFilters(query.AllFilters)

    ...
}
</code></pre>

<p>The search query is much more readable than what we started with in Iteration 1. One thing you should always be careful about with EF is making sure that the generated SQL is optimized and you are across what gets executed on the server and the client. Using a SQL Profiler or <a href="https://docs.microsoft.com/en-us/ef/core/miscellaneous/logging">configure logging</a> to see the generated SQL. You can also <a href="https://docs.microsoft.com/en-us/ef/core/querying/client-eval#optional-behavior-throw-an-exception-for-client-evaluation">configure to throw an exception</a> (in your development environment) for client evaluation.</p>

<p>Hope this helps to write cleaner and readable queries. Sound off in the comments if you have thoughts on refining this further or of any other patterns that you use.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exclude Certain Scripts From Transaction When Using DbUp]]></title>
    <link href="http://rahulpnath.com/blog/exclude-certain-scripts-from-transaction-when-using-dbup/"/>
    <updated>2018-12-17T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/exclude-certain-scripts-from-transaction-when-using-dbup</id>
    <content type="html"><![CDATA[<p>Recently I had written about <a href="https://rahulpnath.com/blog/setting-up-dbup-in-azure-pipelines/">Setting Up DbUP in Azure Pipelines</a> at one of my clients. We had all our scripts run under <a href="https://dbup.readthedocs.io/en/latest/more-info/transactions/">Transaction Per Script</a> mode and was all working fine until we had to deploy some SQL scripts that cannot be run under a transaction. So now I have a bunch of SQL script files that can be run under a transaction and some (like the ones below - <a href="https://azure.microsoft.com/en-au/blog/full-text-search-is-now-available-for-preview-in-azure-sql-database/">Full-Text Search</a>) that cannot be run under a transaction. By default, if you run this using DbUp under a transaction you get the error message<span class="text-danger">
CREATE FULLTEXT CATALOG statement cannot be used inside a user transaction </span> and this is an <a href="https://github.com/DbUp/DbUp/issues/207">existing issue</a>.</p>

<pre><code class="sql Full Text Search Script">CREATE FULLTEXT CATALOG MyCatalog
GO

CREATE FULLTEXT INDEX 
ON  [dbo].[Products] ([Description])
KEY INDEX [PK_Products] ON MyCatalog
WITH CHANGE_TRACKING AUTO
GO
</code></pre>

<p>One option would be to turn off transaction all together using <em>builder.WithoutTransaction()</em> (default transaction setting) and everything would work as usual. But in case you want each of your scripts to be run under a transaction you can choose either of the options below.</p>

<h3>Using Pre-Processors to Modify Script Before Execution</h3>

<p><a href="https://dbup.readthedocs.io/en/latest/more-info/preprocessors/">Script Pre-Processors</a> are an extensibility hook into DbUp and allows you to modify a script before it gets executed. So we can wrap each SQL script with a transaction before it gets executed. In this case, you have to configure your builder to run WithoutTransaction and modify each script file before execution and explicitly wrap with a transaction if required. Writing a custom pre-processor is quickly done by implementing the IScriptPreprocessor interface, and you get the contents of the script file to modify. In this case, all I do is check whether the text contains &lsquo;CREATE FULLTEXT&rsquo; and wrap with a transaction if it does not. You could use file-name conventions or any other rules of your choice to perform the check and conditionally wrap with a transaction.</p>

<pre><code class="csharp Conditionally Apply Transaction">public class ConditionallyApplyTransactionPreprocessor : IScriptPreprocessor
{
    public string Process(string contents)
    {
        if (!contents.Contains("CREATE FULLTEXT", StringComparison.InvariantCultureIgnoreCase))
        {
            var modified =
                $@"
BEGIN TRANSACTION   
BEGIN TRY
           {contents}
    COMMIT;
END TRY
BEGIN CATCH
    ROLLBACK;
    THROW;
END CATCH";

            return modified;
        }
        else
            return contents;
    }
}
</code></pre>

<h3>Using Multiple UpgradeEngine to Deploy Scripts</h3>

<p>If you are not particularly fine with tweaking the pre-processing step and want to use the default implementations of DbUp and still achieve keep transactions for you scripts where possible, you can use multiple upgraders to perform the job for you. Iterate over all your script files and then partition them into batches of files that need to be run under a transaction and those that can&rsquo;t be run under a transaction. As shown in the image below you will end up with multiple batches with alternating sets of transaction/non-transaction set of scripts. When performing the upgrade over a batch, set the <em>WithTransactionPerScript</em> on the builder conditionally. If any of the batches fail, you can terminate the database upgrade.</p>

<p><img src="/images/dbup_batches.png" alt="Script file batches" class="center" /></p>

<pre><code class="csharp Execute all batches (Might not be production ready)">{
    Func&lt;string,bool&gt; canRunUnderTransaction = (fileName) =&gt; !fileName.Contains("FullText");
    Func&lt;List&lt;string&gt;, string, bool&gt; belongsToCurrentBatch = (batch, file) =&gt;
        batch != null &amp;&amp;
        canRunUnderTransaction(batch.First()) == canRunUnderTransaction(file);

    var batches = allScriptFiles.Aggregate
        (new List&lt;List&lt;string&gt;&gt;(), (current, next) =&gt;
            {
                if (belongsToCurrentBatch(current.LastOrDefault(),next))
                    current.Last().Add(next);
                else
                    current.Add(new List&lt;string&gt;() { next });

                return current;
            });

    foreach (var batch in batches)
    {
        var includeTransaction = !batch.Any(canRunUnderTransaction);

        var result = PerformUpgrade(batch.ToSqlScriptArray(), includeTransaction);

        if (!result.Successful)
        {
            Console.ForegroundColor = ConsoleColor.Red;
            Console.WriteLine(result.Error);
            Console.ResetColor();
            return -1;
        }
    }

    Console.ForegroundColor = ConsoleColor.Green;
    Console.WriteLine("Success!");
    Console.ResetColor();
    return 0;
}

private static DatabaseUpgradeResult PerformUpgrade(
    SqlScript[] scripts,
    bool includeTransaction)
{
    var builder = DeployChanges.To
        .SqlDatabase(connectionString)
        .WithScripts(scripts)
        .LogToConsole();

    if (includeTransaction)
        builder = builder.WithTransactionPerScript();

      var upgrader = builder.Build();

    var result = upgrader.PerformUpgrade();

    return result;
}
</code></pre>

<p>Keeping all your scripts in a single place and automating it through the build-release pipeline is <a href="https://rahulpnath.com/blog/working-effectively-under-constraints/">something you need to strive for</a>. Hope this helps you to continue using DbUp even if you want to execute scripts that are a mix of transactional and non-transactional.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up DbUp in Azure Pipelines]]></title>
    <link href="http://rahulpnath.com/blog/setting-up-dbup-in-azure-pipelines/"/>
    <updated>2018-12-03T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/setting-up-dbup-in-azure-pipelines</id>
    <content type="html"><![CDATA[<p><a href="https://azure.microsoft.com/en-au/services/devops/pipelines/">Azure Pipelines</a> is part of the <a href="https://azure.microsoft.com/en-au/services/devops/">Azure Devops</a> offerings which enables you to continuously build test and deploy to any platform and cloud environments. It&rsquo;s been a while since this has been out and it&rsquo;s only recently that I have got a chance to play around with it at one of my clients. We use <a href="https://dbup.readthedocs.io/en/latest/">DBUp</a>, a .Net library to deploy schema changes to our SQL Server database. <em>It tracks which SQL scripts have been run already, and runs the change scripts that are needed to get your database up to date.</em></p>

<p>Setting up DbUp is very easy, and you can use the script straight from the <a href="https://dbup.readthedocs.io/en/latest/">docs</a> to get started. If you are using .Net core console application VS template to setup DbUp make sure to modify the return type of the main function to use int and to return the appropriate application exit codes (as from the script in the doc.) I made the mistake of removing the return statements, only to later realize that build scripts were successfully passing even though the DbUp scripts were failing.</p>

<div class="alert alert-warning">
If you are using the .Net Core console application VS template (like I did) make sure you modify the return type of the main function in Program.cs to int. 
</div>


<p>In Azure Pipelines I have the build step publish the build output as a zip artifact. Using this in the release pipeline is a 2 step process</p>

<h4><strong>1 - Extract Zip Package</strong></h4>

<p>Using the <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/extract-files?view=vsts">Extract Files Task</a> extract the zip package from the build artifacts. You can specify a destination folder for the files to be extracted to (as shown below).</p>

<p><img src="/images/dbup_azure_pipelines_extract.jpg" class ="center" alt="Extract package"></p>

<h4><strong>2 - Execute DbUp Package</strong></h4>

<p>With the package extracted out into a folder, we can now execute the console application (using the <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-run?tabs=netcore21#description">dotnet command line</a>) by passing in the connection string as a command line argument.</p>

<p><img src="/images/dbup_azure_pipelines_execute.jpg" class ="center" alt="Execute package"></p>

<p>You now have your database deployments automated through the Azure Pipelines.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing Your Postman API Specs]]></title>
    <link href="http://rahulpnath.com/blog/manage-your-postman-api-specs/"/>
    <updated>2018-07-02T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/manage-your-postman-api-specs</id>
    <content type="html"><![CDATA[<p>In the previous post, we explored how to use <a href="Automated%20API%20Testing%20Using%20Postman%20Collection%20Runner">Postman for testing API endpoints</a>. Postman is an excellent tool to manage API specs as well, so that you can try API requests individually to see how things are working. It also acts as documentation for all your API endpoints and serves as a good starting point for someone new to the team. When it comes to managing the API specs for your application, there are a few options that we have and let&rsquo;s explore what they are.</p>

<h3>Organizing API Specs</h3>

<p>Postman supports the concept of <a href="https://www.getpostman.com/docs/v6/postman/collections/creating_collections">Collections</a>, which are nothing but a Folder to group of saved API requests/Specs. Collections support nesting which means you can add Folders within a collection to further group them. As you can see below the <em>MyApplication</em> and <em>Postman Echo</em> are collections, and there are subfolders inside them which in turn contains API requests. The multi-level hierarchy helps you to organize your requests the way you want to.</p>

<p><img src="/images/postman_collections.png" alt="Postman Collections" class ="center"></p>

<h3>Sharing API Specs</h3>

<p>Any Collection that you create in Postman is automatically synced to Postman Cloud if you are logged in with an account. It allows you to share collections through a link. With <a href="https://www.getpostman.com/pricing">paid version of Postman</a> you get to create <a href="https://www.getpostman.com/workspaces">team workspaces</a>, which means a team can collaborate on the shared versions. It allows easy sharing of specs across your team and manages them in a centralized place.</p>

<p>However, if you are not logged in or don&rsquo;t have a paid version of Postman, you can maintain the specs along with your Source Code. Postman allows you to <a href="https://www.getpostman.com/docs/v6/postman/collections/sharing_collections#sharing-as-a-file">export Collections and share specs as a JSON file</a>. You can then check this file into your source code repository. Other team members can Import the exported file to get the latest specs. The only disadvantage with this is that you need to make sure to export/import every time you/other team members make a change to the JSON file. However, I have seen this approach work well in teams and one way we made sure that the JSON file was up to date is to have to update the API spec as a Work Item and which required to be <a href="https://rahulpnath.com/blog/code-review/">peer reviewed</a>(through Pull Requests)</p>

<h3>Managing Environments</h3>

<p>Typically any application/API would be deployed to multiple environments (like localhost, Development, Testing, Production, etc.) and you would want to switch between these environments to test your API endpoints seamlessly. Postman makes this easy by using the <a href="https://www.getpostman.com/docs/v6/postman/environments_and_globals/manage_environments">Environment Feature</a>.</p>

<p><img src="/images/postman_environment.png" alt="Postman Environment" class="center" /></p>

<p>Again as with Collections, Environments are also synced to Postman Cloud when you are logged in. It makes all your environments available to all your team seamlessly. However, if you are not logged in you can again export the environments as a JSON file and then share that out of band (in a secure manner as this might have sensitive information like tokens, keys, etc.) with your team.</p>

<h3>Publishing API Specs</h3>

<p>Postman allows you to <a href="https://www.getpostman.com/docs/v6/postman/api_documentation/publishing_public_docs">publish API specs</a> (even to a custom URL), which can act like your API Documentation. You can publish it per environments and also easily execute them. Publishing is available only if you log in to an account as it requires the API Specs and environment details in the first place.</p>

<p><img src="/images/postman_published.png" alt="Postman Published" class="center" /></p>

<h3>Security Considerations</h3>

<p>When using the sync feature of Postman (logged in to the application with Postman account), it is <a href="https://www.getpostman.com/docs/v6/postman_for_publishers/run_button/security">recommended</a> that you do not have any sensitive information (like passwords/tokens) as part of the API request spec/Collection. These should be extracted out as Environment variables and stored as part of the appropriate environment.</p>

<p>If you are logged in, all the data that you add to it is automatically synced, which means it will be living in Postman&rsquo;s cloud server. This might not be a desirable option for every company but looks like there is no option to turn sync off at the Collection level. The only way to not sync collections is to not log into an account in Postman.</p>

<div class="alert alert-warning">
    <i>
    If you are logged into Postman then any collection that you create is automatically synced to Postman server. Only way to <a href="https://support.getpostman.com/hc/en-us/articles/203492852-How-do-I-disable-Sync-">prevent sync</a> is not to log in
    </i>
</div>


<p>We have seen the options by which you can share API collections and environments amongst your team even if you are logged in. However, one thing to be aware of is if any of your team members are logged into Postman and imports a collection shared via Repository/out of band methods, it will be synced to Postman server. So at the organization/team level, you would need ways to prevent this from happening if it is essential for you. Best is to have your API&rsquo;s designed in such a way that you do not have to expose such sensitive information, which anyways is a better practice.</p>

<p>Hope this allows to manage your API specs better!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NDC Security 2018 - Overview and Key Takeaways]]></title>
    <link href="http://rahulpnath.com/blog/ndc-security-2018-overview-and-key-takeaways/"/>
    <updated>2018-05-28T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/ndc-security-2018-overview-and-key-takeaways</id>
    <content type="html"><![CDATA[<p>While in Sydney I was lucky enough to have attended the <a href="https://rahulpnath.com/blog/ndc-sydney/">first</a> and <a href="https://rahulpnath.com/blog/ndc-sydney-2017/">second</a> NDC Conferences. After moving up to Brisbane, did not think I could attend one of these soon. However, then comes a nice shorter version of NDC specific to Security - <a href="https://ndcsecurity.com.au/">NDC Security</a>. As the name suggests, this conference is particular to security-related topics with a 2-day workshop and 1-day conference, as was held in Gold Coast, Queensland.</p>

<h3>The Workshop</h3>

<p><a href="https://www.troyhunt.com/">Troy Hunt</a> and <a href="https://scotthelme.co.uk/">Scott Helme</a> ran <a href="https://ndcsecurity.com.au/workshops/">two workshops</a> and I attended <a href="https://ndcsecurity.com.au/workshop/hack-yourself-first-how-to-go-on-the-cyber-offence/">Hack Yourself First</a> by Troy. The workshop covers a wide range of topics and is perfect for anyone who is into web development. The best thing is that you only need to have a browser and <a href="https://www.telerik.com/download/fiddler">Fiddler</a>/<a href="https://www.charlesproxy.com/download/">Charles Proxy</a> (depending on whether you are on Windows or Mac land). One of the interesting thing about the workshop is that it puts you first into the hackers perspective and forces you to exploit existing vulnerabilities in the <a href="http://hackyourselffirst.troyhunt.com/">sample site</a> designed specifically for this. Once you can do this, we then look at ways of protecting ourselves against such exploits and other mechanisms involved.</p>

<p><img src="/images/ndc_security_hyf.jpg" class="center" alt="Hack yourself first, Troy Hunt"></p>

<p>The workshop highlights how easy it is to find and exploit vulnerabilities in applications. Some tools detect vulnerabilities and exploit them for you if you input a few details to them. You necessarily need not know the vulnerabilities itself or how exactly to exploit them. Such tools make it easy for people to use them on any website that is out there on the web. Combined with the power of search engines it makes it quite easy to make your site vulnerabilities to be easily discoverable.</p>

<h3>The Conference</h3>

<p>There were <a href="https://ndcsecurity.com.au/agenda/">six talks</a> in total and below are the ones that I found interesting.</p>

<ul>
<li><a href="https://ndcsecurity.com.au/talk/csp-xxp-sts-pkp-caa-etc-omg-wtf-bbq/">Scott Helme Talk: CSP XXP STS PKP CAA ETC OMG WTF BBQ…</a></li>
<li><a href="https://ndcsecurity.com.au/talk/dependable-dependencies/">Talk: Dependable Dependencies</a></li>
<li><a href="https://ndcsecurity.com.au/talk/everything-is-cyber-broken/">Everything is Cyber-broken</a></li>
</ul>


<p><img src="/images/ndc_security_conference.jpg" class="center" alt="NDC Securtiy, 2018 - Conference"></p>

<p>The whole web is on a journey towards making it more secure. So it is an excellent time to <a href="https://rahulpnath.com/blog/https-for-free-and-why-you-should-care/">move on to HTTPS</a> if you are not already. Even after enabling HTTPS, it is a good idea to make sure you have got all the appropriate <a href="https://securityheaders.com/">security headers</a> set. Making sure that the libraries that you depend on are patched and updated is equally essential.
There are incidents of massive data breaches because of vulnerabilities in third-party libraries and not keeping them updated.</p>

<blockquote><p><em>Functionality need not be the only reason to upgrade third-party libraries. There might be security vulnerabilities that are getting patched which is an equally good reason to update dependent packages</em></p></blockquote>

<p>The harder thing is to keep track of the vulnerabilities that are getting reported and always checking back with your application&rsquo;s dependencies. There is a wide range of tools that help make this easy and seamlessly integrate within the development workflow. It can be included as early as when a developer intends to include a library into the source code, or in the build pipeline or even for sites that are up and running. The earlier such issues get detected in the software development lifecycle, the less costly and impact it has on time and cost.</p>

<h4><strong>Tools</strong></h4>

<ul>
<li><a href="https://sonarwhal.com/scanner/">Sonarwhal</a></li>
<li><a href="https://snyk.io/">Snyk</a></li>
<li><a href="https://retirejs.github.io/retire.js/">Retire.js</a></li>
<li><a href="https://www.netsparker.com/">Netsparker</a></li>
<li><a href="https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project">OWASP Zed Attack Proxy Project</a></li>
<li><a href="https://www.troyhunt.com/troys-ultimate-list-of-security-links/">Ultimate List of Security Links</a></li>
</ul>


<p>The conference ended with a good discussion between Troy and Scott on how everything is Cyber broken. It touches upon the value of Extended Validation (EV) Certificate and how CA&rsquo;s are trying to push for them while browsers are more and more going away from them. It also touches on various proponents of HTTP and the wrong messages that are getting spread to a broader audience and also about <a href="https://scotthelme.co.uk/revocation-is-broken/">certificate revocations</a> and a lot more. It was a fun discussion and a great end to the three-day event.</p>

<h3>Location and Food</h3>

<p>NDC Security was held at <a href="https://www.qthotelsandresorts.com/gold-coast/">QT Gold Coast, Queensland</a> and well organized. Coffee and drinks were available all throughout the day with a barista on the last day (which was cool). Food was served at start, breaks, and lunch and was good. The conference rooms were great and spacious and had reasonable good internet. Did not face much connectivity issues and everything ran smoothly.</p>

<p><img src="/images/ndc_security_food_location.jpg" class="center" alt="NDC Securtiy, 2018 - Food and Location"></p>

<p>One of the things I first did after coming from the conference was to <a href="https://rahulpnath.com/blog/https-for-free-and-why-you-should-care/">move this blog over to HTTPS</a>. I had been procrastinating long on this, but there were enough reasons to make a move now. Also, there are a bunch of things that catch my eye at client places and other web sites that I visit often. Attending the conference and workshop has been a great value add and recommend to anyone if you have a chance to attend that. For the others, most of the content is available in <a href="https://www.pluralsight.com/">Pluralsight</a>.</p>

<p><em>PS: Special thanks to <a href="https://rahulpnath.com/blog/finding-a-job-abroad/">Readify</a> for sending me to this conference and also providing a &lsquo;paid vacation (accommodation)&rsquo; in Gold Coast. It was a nice three-day break for my wife and son also.</em></p>
]]></content>
  </entry>
  
</feed>
