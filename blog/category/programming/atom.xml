<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/programming/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2017-01-02T00:01:47+00:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Make Your Code Contracts Stronger]]></title>
    <link href="http://rahulpnath.com/blog/stronger-code-contracts/"/>
    <updated>2016-12-12T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/stronger-code-contracts</id>
    <content type="html"><![CDATA[<p>How often have you gone into a class to see the implementation when consuming the class or an interface? I do this almost every other day and it&rsquo;s mostly to check how the code handles boundary conditions. What does it do when there is no value to return, does it need all parameters etc. Reading code is hard and time-consuming, even if it&rsquo;s a code that you yourself have written a few minutes back. Imagine every developer having to go into the implementation detail anytime they consume a class?    Bertrand Meyer in connection with his design of the Eiffel programming language coined the term <a href="https://en.wikipedia.org/wiki/Design_by_contract">Design By Contract</a>, an approach for designing software. The central idea of Design By Contract is to improve the contracts shared between different components in the code base. In this post, we will see how we can improve our C# code and avoid unnecessary guard statements across out code base.</p>

<p><a href="http://nebula.wsimg.com/6e7b8057c7f32b90d4f144424c8a7ae1?AccessKeyId=00F174C5B1CCF865161D&disposition=0&alloworigin=1">
<img style="box-shadow:none;" class="center" alt="Stronger Code Contracts" src="/images/strong_code_contracts.jpg"/>
</a></p>

<h3>Leaky Abstraction</h3>

<p>These days in programming we tend to abstract a lot more than what we really need. <a href="http://www.rahulpnath.com/blog/category/dependency-injection/">Dependency Injection</a> and use of IOC containers have started forcing ourselves to think that everything needs to be an interface. But essentially this is not the case. But the bigger problem lies not in the abstraction, but on depending on the implementation details after abstracting. A <a href="https://en.wikipedia.org/wiki/Leaky_abstraction">leaky abstraction</a> is an abstraction that exposes details and limitations of its underlying implementation to its users that should ideally be hidden away.</p>

<blockquote><p><em>Consuming abstractions assuming a certain implementation is bad practice</em></p></blockquote>

<p>Recently I came across the below code during a code review. Even though an empty string was not a valid configuration value that was not being checked here as the repository implementation returns a null when there is no entry.</p>

<pre><code class="csharp">string config = repository.GetConfig();
if(config == null)
{
    ...
}
</code></pre>

<p>This is a common practice and I have myself fallen for this a lot of times. The fact that the repository returns only a null value is an abstraction detail and is not clear from the contract that it exposes. Anyone could change the repository to start returning an empty string. This will then start failing this code. When taken in isolation the code that uses &lsquo;config&rsquo; must check for null and empty to avoid invalid values. The abstraction contracts (function signatures) must convey whether it always returns a value, whether it can be empty or null. This helps remove unnecessary guarding code or makes guarding mandatory across the code base and also indicates a clear intent.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Robustness_principle">Robustness Principle</a> is a general design guideline for software</p>

<blockquote><p><em>Be conservative in what you do, be liberal in what you accept from others (often reworded as &ldquo;Be conservative in what you send, be liberal in what you accept&rdquo;).</em></p></blockquote>

<p>Applying this principle in this context, we must be conservative in what we return from our function (be it a class or interface) contract. The contract should be as explicit as possible to indicate the nature of values that it returns.</p>

<h3>Stronger Return Types</h3>

<p>A repository returning a string is a weak contract, as it does not clearly express the nature of value it returns. It can return either of these three values - null, an empty string or a valid configuration string. In our application, assuming that null and empty string are invalid we should be having a single representation for this state in the application. C# by its very design encourages us to use this pattern as it embraces the concept of null&rsquo;s - <a href="https://www.linkedin.com/pulse/20141126171912-7082046-tony-hoare-invention-of-the-null-reference-a-billion-dollar-mistake">the billion dollar mistake</a>. But this does not mean we are restricted by it. We can bring in concepts from other languages to help us solve this problem. In F# for example, the Option type represents presence or absence of a value. This is similar to the <a href="https://msdn.microsoft.com/en-us/library/1t3y8s4s.aspx">Nullable type in C#</a>, but not just restricted to value types. <a href="http://fsharpforfunandprofit.com/posts/the-option-type/">Option type</a> is defined as union type with two cases : Some and None. Whenever consuming an option type the compiler forces us to handle both the cases</p>

<blockquote><p><em>In pure F#, <a href="https://fsharpforfunandprofit.com/posts/correctness-exhaustive-pattern-matching/">nulls cannot exist accidentally</a>. A string or object must always be assigned to something at creation, and is immutable thereafter</em></p></blockquote>

<pre><code class="fsharp">let config = getConfig
match config with
| None -&gt; printfn "Invalid config"
| Some c -&gt; printgn "Valid config"
</code></pre>

<p>Though C# does not have anything out of the box to define optional values, we can define one of our own. The <a href="https://github.com/ploeh/Booking/blob/master/BookingDomainModel/Maybe.cs">Maybe</a> class is one such implementation of an optional concept. The name is influenced by the option type in Haskell, <a href="https://wiki.haskell.org/Maybe">Maybe</a>. There are also other implementations of Maybe but the concept remains the same - we can represent an optional type in C#.  The code contracts are stronger using Maybe as a return type. If a function always returns a value, say a string, the function contract should remain as a string. If a function cannot return a value always and can return null/empty (assuming that these are invalid values) then it returns a <em>Maybe<string></em>. This makes it clearer for consuming code on whether they should check for null/empty values.</p>

<pre><code class="csharp">Maybe&lt;string&gt; config = repository.GetConfig();
config.Do(value =&gt; LoadFromFile(value));
</code></pre>

<p>You can write different extension methods on the Maybe class, depending on how you want to process the value. In the above example, I have a Do extension method that calls on to a function with the configuration value if any exists. By explicitly stating that a value may or may not be present we have more clarity in code. No longer do we need any unnecessary null checks in the case where a value is always present. This is best achieved when agreed upon as a convention by the development team and enforced through tooling (like code analysis).</p>

<h3>Value Objects</h3>

<p>One of the root problem for having a lot of null/empty checks scattered across the code is <a href="http://blog.ploeh.dk/2015/01/19/from-primitive-obsession-to-domain-modelling/">Primitive Obsession</a>. Just because you can represent a value as a string, it doesn&rsquo;t mean that you always should. Enforcing structural restrictions imposed by the business is best done by encapsulating these constraints within a class, also known as a <a href="http://www.rahulpnath.com/blog/thinking-beyond-primitive-values-value-objects/">Value Object</a>. This leads to classes for representing various non-nullable values for e.g. Name, configuration, Age etc. You can use this in conjunction with <a href="https://en.wikipedia.org/wiki/Null_Object_pattern">Null Object</a> pattern if required. A value object is a class whose equality is based on the value that it holds. So two class instances with same values will be treated equally. In F# you get this by default but in C# you need to override Equals and GetHashCode functions to enforce this equality.</p>

<pre><code class="csharp">public class Configuration
{
    private string configuration;

    public Configuration(string configuration)
    {
        if (string.IsNullOrEmpty(configuration))
            throw new ArgumentNullException(nameof(configuration), "Configuration value cannot be null");

        this.configuration = configuration;
    }

    // override Equals and GetHashCode
}
</code></pre>

<p>Modeling concepts in the domain as classes helps you to contain the domain/business constraints in a single place. This prevents the need to have null checks elsewhere in the code. Value objects being immutable helps enforce class invariants.</p>

<p>The above two methods help create a stronger contract in code. As with any conventions, this is useful only when followed by the whole team. Conventions are best followed if enforced through tooling. You can create custom code analysis rules to enforce return type to be of type if any method is returning null. Even if you are introducing this into a large existing code base you can do this incrementally, by starting to enforce them on commits (if you are using git) like when <a href="http://www.rahulpnath.com/blog/introducing-code-formatting-into-a-large-codebase/">introducing styling into an existing project</a>. What other contracts do you find helpful to make the code more expressive?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solve the Business Problem, Don't Mimic The Process]]></title>
    <link href="http://rahulpnath.com/blog/solve-the-business-problem-dont-mimic-the-process/"/>
    <updated>2016-12-06T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/solve-the-business-problem-dont-mimic-the-process</id>
    <content type="html"><![CDATA[<p><a href="http://www.dhc-gmbh.com/en/kompetenzen/business-process-management/"><img class="center" alt="Business" src="/images/business_problem_automation.jpg"/></a></p>

<p>While working with clients I often get into conversations with the domain experts and people involved directly with the business. The discussions usually happen around what the process they are currently doing and how to automate those. Knowing the business process is helpful but getting influenced by that to design the solution is often not effective. Recently our team was in a conversation with a domain expert for a new feature request.</p>

<div class="alert alert-warning">
The series of events mentioned below were modified to fit into the conversational style used in the post.
</div>


<blockquote><p><em><strong>Domain Expert</strong> We need to charge customers a processing fee if they pay using an electronic payment method. Depending on the type of card (mastercard, visa etc) the processing charge percentage differs. The processing fees are always charged in the subsequent billing period after their current payment. For e.g. if a customer pays 1000$ for the month of November, then his December bill will have 2% card processing charge in the December invoice</em></p>

<p><em><strong>Team</strong> That sounds easy, think we have enough details to get started on this. Thank you.</em></p>

<p><em><strong>Domain Expert</strong> Perfect. Ahhh&hellip; Before you go, I think this can be a <a href="http://hangfire.io/">hangfire job</a> that runs on 29 every month, a few days before the billing date, 3rd, and generate these charges for the client. This is what we do manually at present. (And walks off)</em></p>

<p><em><strong>Team</strong> Discussing amongst themselves the team agreed that creating a recurring job is the way to go. Based on the assumption that this job will be run only once a month, the job was to read all the invoices from 29th of the previous month till 28th of the current month and charge the clients. The meeting was dismissed and off went everyone busy to get the new feature out</em></p></blockquote>

<h3>Business has Exceptions</h3>

<p>Problems started coming up the immediate month of feature deployment. Below is the sequence of events that happened.</p>

<ul>
<li><strong>29th</strong> : Nice work, team! The processing charges have been applied as expected.</li>
<li><strong>30th</strong> : Some of the invoices have wrong data. We have deleted them. Can you run the job?</li>
<li><strong>2nd</strong> : A few of our clients (as usual) paid late and we need to charge their processing fees. Can you run the job?</li>
<li><strong>15th</strong> : One of our clients is ending tomorrow, so we need to send them an invoice and it should include the processing fees for their last payment. Can you run the job?</li>
</ul>


<p>But wait! We had decided that we will run this job only once a month and that is the only time we need to process the charges. We cannot run that job over and over again.</p>

<blockquote><p><em>What I’ve noticed over the years is that our users find very creative ways to achieve their business objectives despite the limitations of the system that they’re working with. We developers ultimately see these as requirements, but they are better interpreted as <strong>workarounds</strong>.</em></p>

<p><strong><em>- <a href="http://udidahan.com/2013/04/28/queries-patterns-and-search-food-for-thought/">Udi Dahan</a></em></strong></p></blockquote>

<p>The business was right when it said that <em>&lsquo;This is what we do manually at present.</em> What they did not say though is that there were always exceptions. And in these cases, they did the same process, but just for those exceptions. Business process mostly will be around the majority of the cases and the exceptions always get handled ad-hoc. So for the business it&rsquo;s always that which takes a good part of their time that matters more.</p>

<h3>Finding the Way Out</h3>

<p>The problem, in this case, was that the team modeled the solution exactly as the business did manually. Think kind of a solution is most likely to fail in case of exceptions. The human brain can easily deal with these exceptions. But for a program to solve it that way it needs to be told so, which implies that there need to be alternate flow paths defined. So with the improved understanding of these exception cases, the team does another analysis through the problem. After some discussion the team re-defines their original problem statement - *We need to be able to run the job any number of times and it should have the same effect.</p>

<blockquote><p><em>A payment should get </em>one and only one<em> processing charge associated, no matter however times it is seen by the job.</em></p></blockquote>

<p>With the new implementation, we decided to maintain a list of payments (a strong identifier that does not change) we have seen and processed. So every time a payment is seen, it is matched to see if it is already processed. If a charge is not already applied, a charge is applied and added to the list of processed payments. This ensures that they can run the job anytime. The team added in capability to specify the time range to look for invoices. By default, this ranged from 29th - 28th. The team also added in a way to void out payment charges applied, so that whenever the invoices changed then can just clear that off and re-run the job. These changes gave the flexibility to meet the businesses exception cases.</p>

<h3>Idempotent</h3>

<blockquote><p><em>The term <a href="http://www.enterpriseintegrationpatterns.com/patterns/messaging/IdempotentReceiver.html">idempotent</a> is used in mathematics to describe a function that produces the same result if it is applied to itself, i.e. f(x) = f(f(x)). In Messaging this concept translates into a message that has the same effect whether it is received once or multiple times. This means that a message can safely be re-sent without causing any problems even if the receiver receives duplicates of the same message.</em></p></blockquote>

<p>Being idempotent is what we missed with the first implementation. There was an assumed &lsquo;idempotency&rsquo; that the job will be run only once a month. But this constraint is not something that the code had control of and something it could enforce. The job was also not idempotent at the granular level that it was affecting - payments. Asserting idempotency at the batch level fails when we want to re-run batches (when exceptions like the wrong invoice happens). Idempotency should be enforced at the unit level of change, which is what maintaining a list of processed payments helps with. Any payment that is not processed before will get processed now when the job is run. We can also ensure that the payment will only be processed at most once.</p>

<p>This is just an example where we fail to see beyond the business problem and also see the computing problems accompanying it. Not always will it be easy and fast to rewrite the code. Even if we fail to see these problems the business will eventually make us to. But it is when we can see the computing problems that accompanies a business problem that we start becoming better developers. Applying basic computing principles, probing the domain expert during discussions, sitting with domain experts while they work etc. are all good ways to start seeing the untold business processes. Hope this helps the next time you are into a meeting with domain expert or solving a business problem.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Code Formatting into a Large Code Base]]></title>
    <link href="http://rahulpnath.com/blog/introducing-code-formatting-into-a-large-codebase/"/>
    <updated>2016-10-03T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/introducing-code-formatting-into-a-large-codebase</id>
    <content type="html"><![CDATA[<p>Code Formatting is an important aspect of writing code. If followed well it helps keep the code more readable and easy to work with. Here are some of the different aspects of formatting code and my personal preferences. I then explore options to enforce code formatting and ways to introduce it into an existing code base.</p>

<p>Below are some of the popular formatting rules and those that have a high value when enforced in a project.</p>

<h4><strong>Tabs vs Spaces</strong></h4>

<p>One of the most debated topic in code formatting is whether to use tabs or spaces to intend code. I never knew such a debate existed until my most recent project. It had developers from different parts of the world and with different preferences. I came across the below excerpt from Jeff Atwood, to which I completely agree.</p>

<blockquote><p><em>Choose tabs, choose spaces, choose whatever layout conventions make sense to you and your team. It doesn&rsquo;t actually matter which coding styles you pick. What does matter is that you, and everyone else on your team, <strong>sticks with those conventions and uses them consistently.</strong></em></p>

<p><em>That said, only a moron would use tabs to format their code.</em></p>

<p><em>- <a href="https://blog.codinghorror.com/death-to-the-space-infidels/">Jeff Atwood</a></em></p></blockquote>

<p>Settings for these are often available at the IDE level. In Visual Studio this is available under <a href="https://msdn.microsoft.com/en-us/library/7sffa753.aspx">Options, Text Editor, All Languages, Tabs</a>. Be aware of what you choose and make sure you have the same settings across your team members.</p>

<h4><strong>Horizontal Alignment</strong></h4>

<p>Avoid aligning by common separators (=;,) when they occur in adjacent lines. This kind of alignment falls out of order when we rename variables or properties. It happens when you chaange property names.</p>

<pre><code class="csharp Not Refactoring friendly and needs extra effort to keep it formatted">var person = new Person()
{
    FirstName = "Rahul",
    LastName  = "Nath",
    Site      = "www.rahulpnath.com"
};
</code></pre>

<pre><code class="csharp Refactoring friendly">var person = new Person()
{
    FirstName = "Rahul",
    LastName = "Nath",
    Site = "www.rahulpnath.com"
};
</code></pre>

<h4><strong>Horizontal Formatting</strong></h4>

<p><em>You should never have to scroll to the right</em> - I caught on with this recommendation from the book Clean Code (<a href="http://www.rahulpnath.com/blog/language-agnostic-books-for-every-developer-2/">a recommended read</a>). It is also recommended that a function should fit on the screen, without needing to scroll up or down. This encourages to keep functions short and specific.</p>

<blockquote><p><em>We should strive to keep our lines short. The old Hollerith limit of 80 is a bit arbitrary, and I’m not opposed to lines edging out to 100 or even 120. But beyond that is probably just careless</em></p>

<p><em>- Uncle Bob</em></p></blockquote>

<p>The <a href="https://visualstudiogallery.msdn.microsoft.com/d0d33361-18e2-46c0-8ff2-4adea1e34fef">Productivity Power Tools</a> extension for Visual Studio allows adding a Column Guide. A Column Guide reminds developers their full line of code or comments may not fit on a single screen.</p>

<p><img class="center" alt="Code Formatting Maximum Width Column Guide in Visual Studio using Power Tools"
src="/images/codeformatting_column_guide.png" /></p>

<h4><strong>Aligning Function Parameters</strong></h4>

<p>Always try to keep the number of parameters as less as possible. In cases where there are more parameters or longer function names, the team must choose a style. There are different styling formats followed when splitting parameters to a new line.</p>

<p>Allowing parameters to take the <strong><em>natural flow of IDE</em></strong> (Visual Studio) is the simplest approach. This often leads to poor readability and code cluttering.</p>

<p><img class="center" alt="Function Parameters taking natural flow of IDE"
src="/images/codeformatting_functionparameters_naturalflowide.png" /></p>

<p>Breaking parameters into separate lines is important for readability. Use the Column guide to decide when to break function parameters into different lines. There are different approaches followed when splitting parameters into new lines. Keeping the first parameter on the same line as the function and then having all other <strong><em>parameters on new line aligned with the first parameter</em></strong> is another approach. This works well when viewed in the same font and resolution used when writing. When you change font or resolution this kind of formatting falls out of place.</p>

<p><img class="center" alt="Function Parameters on new line aligned with first parameter"
src="/images/codeformatting_functionparameters_alignzoom.png" /></p>

<p>A better variant of the above style is to have the parameters in the new line aligned to the left. This ensures parameters stay in the same place when changing font or resolutions. The one that I prefer is to have all parameters in a new line. This formatting works well with different font sizes and resolutions.</p>

<pre><code class="csharp">public int ThisIsALongFunctionNameWithLotsOfParameters(
    int parameter1, 
    string parameter2, 
    int parameter3, 
    string optionalParameter = "Test")
{
}
</code></pre>

<h4><strong>Visibility Based Ordering</strong></h4>

<p>It is a good practice to maintain a specific order of items within a class. Have all properties declared first, then constructors, public methods, protected methods, private methods etc. This is up to the team to  determine the order, but sticking on to it makes the code more readable.</p>

<h3>Code Analysis Tools</h3>

<p>Checking for styling and formatting issues in a code review requests is a boring task. It’s best to automate style checks at build time (local and server builds). Making build throw errors for styling issues forces developers to fix them. Once developers get used to the rules, writing code without any formatting issues becomes second nature.  <a href="https://stylecop.codeplex.com">StyleCop</a> is an open source static code analysis tool from Microsoft that checks C# code for conformance to StyleCop&rsquo;s recommended coding styles and a subset of Microsoft&rsquo;s .NET Framework Design Guidelines. It has a Visual Studio plugin and also integrates well with <a href="https://stylecop.codeplex.com/wikipage?title=Setting%20Up%20StyleCop%20MSBuild%20Integration">MsBuild</a>.</p>

<h4><strong>Cleaning up a Large Code Base</strong></h4>

<p>Introducing StyleCop (or any code format enforcement) into a large pre-existing code base is challenging. Turning the tool on would immediately throw hundreds and thousands of errors. Trying to fix them in a stretch might impact the ongoing development process. This often causes us to delay the introduction of such enforcement&rsquo;s into the project and it continues to be a technical debt.</p>

<p>Taking an incremental approach, fixing one by one as and when a file is changed seems a good idea. Teams can come with the <a href="http://programmer.97things.oreilly.com/wiki/index.php/The_Boy_Scout_Rule">Boy Scout Rule</a> - &lsquo;<em>Leave the file cleaner than you find</em>&rsquo;. Every time a file is touched for a fix, run StyleCop analysis and fix the errors. Over a period of time, this will make the project clean. The only problem with this approach is developers often tend to ignore/forget running the analysis and fix them.</p>

<blockquote><p><em>Trivial things like code formatting is hard to mandate within a team unless it is enforced through tooling</em></p></blockquote>

<h3>Source Control Hooks</h3>

<p>We can plug into various hooks that source controls give to enforce code formatting on developer machines. In git, you can add a <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks">custom pre-commit hook</a> to run the StyleCop analysis on all the staged files. <a href="https://github.com/bbadjari/stylecopcli">StyleCopcli</a> is an open source application that wraps over the StyleCop DLLs and allows running the analysis from the command line. So in the hook, I use this CLI to run StyleCop analysis on all the staged files.</p>

<pre><code class="bat">#!/bin/sh
echo "Running Code Analysis"
./stylecopcli/StyleCopCLI.exe -cs $(git diff --cached --name-only)
if [ $? = 2 ]
    then
        echo Commit Failed! Fix StyleCop Errors
        exit 1
    else
        echo No SyleCop Errors!
        exit 0
fi
</code></pre>

<p>If there are any StyleCop validation errors the commit is aborted, forcing the developer to fix it. The git hooks work fine when committing from a command line or UI tools like Source Tree. However, Visual Studio git plugin does not run the git hooks and fails to do the check.</p>

<p><img class="center" alt="StyleCop git hook failing commit in console" src="/images/code_formatting_git_hook_console.png" /></p>

<p><img class="center" alt="StyleCop git hook failing commit in Source Tree" src="/images/code_formatting_git_hook_sourcetree.png" /></p>

<p>Over a period of time, most of the files will get cleaned and remaining can be done all at once with less effort. Once the entire code base passes all StyleCop rules, this can be enforced in the build server. This ensures that no more bad formatted code gets checked into the source control.</p>

<p>Code is read more than written. So it is important to keep it readable and well-formatted. It also makes navigating code bases easier and faster. These are minor things that are often <a href="https://vimeo.com/97329157">overlooked by developers</a>, but have a high impact on productivity when followed. Do you enforce code formatting rules in your current project? What are the rules that you find important. Sound off in the comments below!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Keeping Sensitive Configuration Data Out of Source Control]]></title>
    <link href="http://rahulpnath.com/blog/keeping-sensitive-configuration-data-out-of-source-control/"/>
    <updated>2016-09-19T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/keeping-sensitive-configuration-data-out-of-source-control</id>
    <content type="html"><![CDATA[<p>Most applications today deals with some form of sensitive information. The most commonly seen are database connection strings, API keys, token etc. The web.config seems the best place to have these values, but it definitely is not. In most cases it gets pushed into the source control systems as well. If it is a private repository then you at least have one level of security on top of it. It still exposes sensitive information to anyone who has access to the repository. It’s worse when the <a href="http://www.internetnews.com/blog/skerner/github-search-exposes-passwords.html">repository is public</a>.</p>

<p><img alt="Keep sensitive data out of source control" src="/images/sensitivedata_source_control.png" /></p>

<p>There are different ways you can  avoid pushing sensitive data into source control. In this post, I will explore options that I am familiar with.</p>

<blockquote><p><em>Use configuration files as template definitions for the configuration data your application requires. Have the actual values stored elsewhere</em></p></blockquote>

<h3>Azure App Settings</h3>

<p>If you are deploying your application as a Web App on Azure, you can store <a href="https://azure.microsoft.com/en-us/blog/windows-azure-web-sites-how-application-strings-and-connection-strings-work/">application settings and connection strings in Azure</a>. At runtime, Windows Azure Web Sites automatically retrieves these values for you and makes them available to code running in your website. This removes the need for having sensitive data in the configuration file.</p>

<p><img alt="Azure App Settings and Connection Strings" src="/images/sensitiveData_azure_app_settings.png" /></p>

<h3>Release Management Tools</h3>

<p>Release management tools like Octopus Deploy, Microsoft Release Management, that performs configuration transformation. It supports creating different environments (development, production) and corresponding configurations. On creating a package for an environment, it applies the corresponding environment configurations</p>

<p><img alt="Release Management Tools - Octopus Deploy" src="/images/sensitiveData_releaseManagement_tool_octopus.png" /></p>

<p>Packaging embeds the configuration value into the configuration file. This makes it available to anyone who has access to the host systems.</p>

<h3>Azure Key Vault</h3>

<p>Azure Key Vault acts as a centralized repository for all sensitive information. Key vault stores cryptographic keys and Secrets and makes them available over a HTTP Api. The objects (keys and secrets) in key vault has unique identifier to retrieve them. Check <a href="http://www.rahulpnath.com/blog/azure-key-vault-in-a-real-world-application/">Azure Key Vault in real world application</a> for more details on how to achieve this. A client application can <a href="http://www.rahulpnath.com/blog/authenticating-a-client-application-with-azure-key-vault/">authenticate with Azure Key Vault using a ClientID/secret or ClientID/certificate</a>. Using certificate to authenticate is the preferred approach. To get Keys/Secret from key vault all you need is the AD Application Id, the client secret or certificate identifier and the key/secret names. The certificate itself can be deployed separately into the application host.</p>

<pre><code class="XML">&lt;appSettings&gt;
  &lt;add key="KeyVaultUrl" value="https://testvaultrahul.vault.azure.net"/&gt;
  &lt;add key="ADApplicationId" value="" /&gt;
  &lt;add key="ADCertificateThumbprint" value="" /&gt;
  &lt;add key="DbConnectionString" value="SqlConnectionString"/&gt;
  &lt;add key ="ApiToken" value="ApiToken/cfedea84815e4ca8bc19cf8eb943ee13"/&gt;
&lt;/appSettings&gt;
</code></pre>

<p>If you are using the &lsquo;client secret&rsquo; to authenticate then the configuration file will have the Secret. In either cases, you should follow either of the previous approaches to keep the Application Id and authentication information out of configuration. The advantage of using <a href="http://www.rahulpnath.com/blog/category/azure-key-vault/">Key Vault</a> is that it is a centralized  repository for all your sensitive data, across different applications. You can also restrict access permissions per application.</p>

<p>These are some approaches to keep sensitive information out of source control. What approach do you use? Irrespective of the approach you use, make sure that you don’t check them in!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Hotfix : Things to Remember]]></title>
    <link href="http://rahulpnath.com/blog/data_hotfix/"/>
    <updated>2016-08-25T04:27:32+00:00</updated>
    <id>http://rahulpnath.com/blog/data_hotfix</id>
    <content type="html"><![CDATA[<p>Yesterday I was late to leave office as I had to data fix some of the systems that we are currently building. We had just migrated few hundred clients onto the new platform. Invoices generated for the clients had wrong invoice amounts due to some mismatching data used when migrating. We had the expected invoice from the old system which made finding the problem easy. We ran a few scripts to correct the data in different systems and fixed the issue.</p>

<p><a href="http://static1.squarespace.com/static/54652521e4b0045935420a6c/t/548dee03e4b0f1b25cb560d5/1418587652009/Data.jpg?format=1500w">
    <img class="center" alt="Data Hotfix" src="/images/data_hotfix.jpg" />
</a></p>

<div class="alert alert-warning">
<strong>WARNING!</strong> Normally I do not recommend making any changes directly in production server. In this case, there was a business urgency and was forced to do the data fix the same night, for smooth functioning the day after. We still managed to get in some testing in the development environment before running it in production.
</div>


<h3>It All Starts with a Few</h3>

<p>I have seen it repeatedly happen that this kind of data fixes starts with a few in the beginning. Within a short span of time the affected data size grows drastically and manual updates might not be a good solution.</p>

<blockquote><p><em>If you get a second thought of whether to script the fix or not, then you should script it.</em></p></blockquote>

<p> Yesterday it started with data fix for 30 clients and the fix was relatively small. It could either be through UI or API. Fix through the UI took around 45 seconds each, and there were two of us. So it was just a matter of 12-15 minutes to fix it. While fixing, one of us found an extra scenario where the same fix needs to be applied. Re-running the query to find such clients bombarded the number to 379. At this moment, I stood up and said <em>I am going to script this. There is no way  I am doing this manually.</em> Manually fixing this would take five man hours, but will finish in two and half hours, as there were two of us. Even writing the script is going to take around an hour but that&rsquo;s just one man hour.</p>

<blockquote><p><em>There is happiness you get when you script the fix and not manually plow through the UI fixing each of them</em></p></blockquote>

<p>The script was in C#, written as a test case, invoked from a test runner (which I don&rsquo;t feel great about now) updating the systems with the data fix. It did its job and fixed all the cases it was supposed to. But I was not happy with the approach that I had chosen to make the fix. Correcting production data through a unit test script does not sound a robust solution. The reason to choose tests was that the test project had all the code required to access the other systems. It was just about changing the configuration values to point to the production system. It was the shortest path to having at least one client updated and verified.</p>

<p>Having it as a test script restricted me from scaling the update process (though I could have done some fancy things to <a href="https://xunit.github.io/docs/running-tests-in-parallel.html">run tests in parallel</a>). It also forced me to hard-code the input data.Logging was harder  and I used <a href="https://msdn.microsoft.com/en-us/library/system.diagnostics.debug.writeline(v=vs.110).aspx">Debug.WriteLine</a> to the VS output window. All those were the aftermath of choosing the wrong execution method - running it as a test script!</p>

<p>In retrospective, here are a few things that I should have done different and should be doing if ever I am in a similar situation again.</p>

<h4><strong>Create Stand-alone Executable</strong></h4>

<p>Having a stand-alone executable running the script provides the capability to scale the number of processes as I wanted. Input can be passed as a file or as an argument to the application allowing to break the large data set into smaller subsets.</p>

<h4><strong>Log Error and Success</strong></h4>

<p>It&rsquo;s very much possible that the &lsquo;fix-to-fix errors&rsquo; can go wrong or throw exceptions. So handle for errors and log appropriate message to take any corrective actions. It&rsquo;s better to log to a file or other durable storage as that is more foolproof. Logging to the output window (Debug.Writeline/Console.Writeline) is not recommended, as there is a risk of accidentally losing it (with another test run or closing VS).</p>

<p> Logging successes are equally important to keep track of fixed records. It helps in cases where the process terminates suddenly while processing a set of data. It gives a track of all data sets that were successfully processed and exclude from following runs.</p>

<h4><strong>Test</strong></h4>

<p>It is very likely that the script has bugs and does not handle all possible cases. So as with any code, testing the data fix script is also mandatory. Preferably, test in a development/test environment, if not try for a small subset of input in the production. In my case, I was able to test in the development environment and then in production. But still, I ran a small subset in production first and ended up finding an issue that I could not find in development.</p>

<h4><strong>Parallelize if Possible</strong></h4>

<p>In cases where the data fixes are independent of each other (which likely is when dealing with large data fixes), each of the updates can be in parallel. Also using nonblocking calls when updating across the network helps speed up the process, by reducing the idle time and improves the overall processing time.</p>

<h4><strong>Parameterize Input</strong></h4>

<p>Parameterizing of input to the script (console) application helps when you want to scale the application. In my case updating each of the clients took around 8-10 seconds as it involved calling multiple geographically distributed systems. (Updating a system in the US from Australia does take a while!). Having a parameterized application enables to have multiple applications running with different input sets updating the data and speeds up the overall processing time.</p>

<p>It&rsquo;s hard to come up with a solid plan for critical data fixes. It might not be possible to follow all of the points above. Also, there might be a lot other things to be done other than these. These are just a few things for reference so that I can stop, take a look and move on when a similar need arises.  Hope this helps someone else too! Drop in a comment if you have any tips for the &lsquo;eleventh hour&rsquo; fix!</p>
]]></content>
  </entry>
  
</feed>
