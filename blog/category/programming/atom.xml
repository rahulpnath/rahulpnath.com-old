<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/programming/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2018-10-18T00:02:00+00:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Managing Your Postman API Specs]]></title>
    <link href="http://rahulpnath.com/blog/manage-your-postman-api-specs/"/>
    <updated>2018-07-02T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/manage-your-postman-api-specs</id>
    <content type="html"><![CDATA[<p>In the previous post, we explored how to use <a href="Automated%20API%20Testing%20Using%20Postman%20Collection%20Runner">Postman for testing API endpoints</a>. Postman is an excellent tool to manage API specs as well, so that you can try API requests individually to see how things are working. It also acts as documentation for all your API endpoints and serves as a good starting point for someone new to the team. When it comes to managing the API specs for your application, there are a few options that we have and let&rsquo;s explore what they are.</p>

<h3>Organizing API Specs</h3>

<p>Postman supports the concept of <a href="https://www.getpostman.com/docs/v6/postman/collections/creating_collections">Collections</a>, which are nothing but a Folder to group of saved API requests/Specs. Collections support nesting which means you can add Folders within a collection to further group them. As you can see below the <em>MyApplication</em> and <em>Postman Echo</em> are collections, and there are subfolders inside them which in turn contains API requests. The multi-level hierarchy helps you to organize your requests the way you want to.</p>

<p><img src="/images/postman_collections.png" alt="Postman Collections" class ="center"></p>

<h3>Sharing API Specs</h3>

<p>Any Collection that you create in Postman is automatically synced to Postman Cloud if you are logged in with an account. It allows you to share collections through a link. With <a href="https://www.getpostman.com/pricing">paid version of Postman</a> you get to create <a href="https://www.getpostman.com/workspaces">team workspaces</a>, which means a team can collaborate on the shared versions. It allows easy sharing of specs across your team and manages them in a centralized place.</p>

<p>However, if you are not logged in or don&rsquo;t have a paid version of Postman, you can maintain the specs along with your Source Code. Postman allows you to <a href="https://www.getpostman.com/docs/v6/postman/collections/sharing_collections#sharing-as-a-file">export Collections and share specs as a JSON file</a>. You can then check this file into your source code repository. Other team members can Import the exported file to get the latest specs. The only disadvantage with this is that you need to make sure to export/import every time you/other team members make a change to the JSON file. However, I have seen this approach work well in teams and one way we made sure that the JSON file was up to date is to have to update the API spec as a Work Item and which required to be <a href="https://rahulpnath.com/blog/code-review/">peer reviewed</a>(through Pull Requests)</p>

<h3>Managing Environments</h3>

<p>Typically any application/API would be deployed to multiple environments (like localhost, Development, Testing, Production, etc.) and you would want to switch between these environments to test your API endpoints seamlessly. Postman makes this easy by using the <a href="https://www.getpostman.com/docs/v6/postman/environments_and_globals/manage_environments">Environment Feature</a>.</p>

<p><img src="/images/postman_environment.png" alt="Postman Environment" class="center" /></p>

<p>Again as with Collections, Environments are also synced to Postman Cloud when you are logged in. It makes all your environments available to all your team seamlessly. However, if you are not logged in you can again export the environments as a JSON file and then share that out of band (in a secure manner as this might have sensitive information like tokens, keys, etc.) with your team.</p>

<h3>Publishing API Specs</h3>

<p>Postman allows you to <a href="https://www.getpostman.com/docs/v6/postman/api_documentation/publishing_public_docs">publish API specs</a> (even to a custom URL), which can act like your API Documentation. You can publish it per environments and also easily execute them. Publishing is available only if you log in to an account as it requires the API Specs and environment details in the first place.</p>

<p><img src="/images/postman_published.png" alt="Postman Published" class="center" /></p>

<h3>Security Considerations</h3>

<p>When using the sync feature of Postman (logged in to the application with Postman account), it is <a href="https://www.getpostman.com/docs/v6/postman_for_publishers/run_button/security">recommended</a> that you do not have any sensitive information (like passwords/tokens) as part of the API request spec/Collection. These should be extracted out as Environment variables and stored as part of the appropriate environment.</p>

<p>If you are logged in, all the data that you add to it is automatically synced, which means it will be living in Postman&rsquo;s cloud server. This might not be a desirable option for every company but looks like there is no option to turn sync off at the Collection level. The only way to not sync collections is to not log into an account in Postman.</p>

<div class="alert alert-warning">
    <i>
    If you are logged into Postman then any collection that you create is automatically synced to Postman server. Only way to <a href="https://support.getpostman.com/hc/en-us/articles/203492852-How-do-I-disable-Sync-">prevent sync</a> is not to log in
    </i>
</div>


<p>We have seen the options by which you can share API collections and environments amongst your team even if you are logged in. However, one thing to be aware of is if any of your team members are logged into Postman and imports a collection shared via Repository/out of band methods, it will be synced to Postman server. So at the organization/team level, you would need ways to prevent this from happening if it is essential for you. Best is to have your API&rsquo;s designed in such a way that you do not have to expose such sensitive information, which anyways is a better practice.</p>

<p>Hope this allows to manage your API specs better!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NDC Security 2018 - Overview and Key Takeaways]]></title>
    <link href="http://rahulpnath.com/blog/ndc-security-2018-overview-and-key-takeaways/"/>
    <updated>2018-05-28T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/ndc-security-2018-overview-and-key-takeaways</id>
    <content type="html"><![CDATA[<p>While in Sydney I was lucky enough to have attended the <a href="https://rahulpnath.com/blog/ndc-sydney/">first</a> and <a href="https://rahulpnath.com/blog/ndc-sydney-2017/">second</a> NDC Conferences. After moving up to Brisbane, did not think I could attend one of these soon. However, then comes a nice shorter version of NDC specific to Security - <a href="https://ndcsecurity.com.au/">NDC Security</a>. As the name suggests, this conference is particular to security-related topics with a 2-day workshop and 1-day conference, as was held in Gold Coast, Queensland.</p>

<h3>The Workshop</h3>

<p><a href="https://www.troyhunt.com/">Troy Hunt</a> and <a href="https://scotthelme.co.uk/">Scott Helme</a> ran <a href="https://ndcsecurity.com.au/workshops/">two workshops</a> and I attended <a href="https://ndcsecurity.com.au/workshop/hack-yourself-first-how-to-go-on-the-cyber-offence/">Hack Yourself First</a> by Troy. The workshop covers a wide range of topics and is perfect for anyone who is into web development. The best thing is that you only need to have a browser and <a href="https://www.telerik.com/download/fiddler">Fiddler</a>/<a href="https://www.charlesproxy.com/download/">Charles Proxy</a> (depending on whether you are on Windows or Mac land). One of the interesting thing about the workshop is that it puts you first into the hackers perspective and forces you to exploit existing vulnerabilities in the <a href="http://hackyourselffirst.troyhunt.com/">sample site</a> designed specifically for this. Once you can do this, we then look at ways of protecting ourselves against such exploits and other mechanisms involved.</p>

<p><img src="/images/ndc_security_hyf.jpg" class="center" alt="Hack yourself first, Troy Hunt"></p>

<p>The workshop highlights how easy it is to find and exploit vulnerabilities in applications. Some tools detect vulnerabilities and exploit them for you if you input a few details to them. You necessarily need not know the vulnerabilities itself or how exactly to exploit them. Such tools make it easy for people to use them on any website that is out there on the web. Combined with the power of search engines it makes it quite easy to make your site vulnerabilities to be easily discoverable.</p>

<h3>The Conference</h3>

<p>There were <a href="https://ndcsecurity.com.au/agenda/">six talks</a> in total and below are the ones that I found interesting.</p>

<ul>
<li><a href="https://ndcsecurity.com.au/talk/csp-xxp-sts-pkp-caa-etc-omg-wtf-bbq/">Scott Helme Talk: CSP XXP STS PKP CAA ETC OMG WTF BBQâ€¦</a></li>
<li><a href="https://ndcsecurity.com.au/talk/dependable-dependencies/">Talk: Dependable Dependencies</a></li>
<li><a href="https://ndcsecurity.com.au/talk/everything-is-cyber-broken/">Everything is Cyber-broken</a></li>
</ul>


<p><img src="/images/ndc_security_conference.jpg" class="center" alt="NDC Securtiy, 2018 - Conference"></p>

<p>The whole web is on a journey towards making it more secure. So it is an excellent time to <a href="https://rahulpnath.com/blog/https-for-free-and-why-you-should-care/">move on to HTTPS</a> if you are not already. Even after enabling HTTPS, it is a good idea to make sure you have got all the appropriate <a href="https://securityheaders.com/">security headers</a> set. Making sure that the libraries that you depend on are patched and updated is equally essential.
There are incidents of massive data breaches because of vulnerabilities in third-party libraries and not keeping them updated.</p>

<blockquote><p><em>Functionality need not be the only reason to upgrade third-party libraries. There might be security vulnerabilities that are getting patched which is an equally good reason to update dependent packages</em></p></blockquote>

<p>The harder thing is to keep track of the vulnerabilities that are getting reported and always checking back with your application&rsquo;s dependencies. There is a wide range of tools that help make this easy and seamlessly integrate within the development workflow. It can be included as early as when a developer intends to include a library into the source code, or in the build pipeline or even for sites that are up and running. The earlier such issues get detected in the software development lifecycle, the less costly and impact it has on time and cost.</p>

<h4><strong>Tools</strong></h4>

<ul>
<li><a href="https://sonarwhal.com/scanner/">Sonarwhal</a></li>
<li><a href="https://snyk.io/">Snyk</a></li>
<li><a href="https://retirejs.github.io/retire.js/">Retire.js</a></li>
<li><a href="https://www.netsparker.com/">Netsparker</a></li>
<li><a href="https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project">OWASP Zed Attack Proxy Project</a></li>
<li><a href="https://www.troyhunt.com/troys-ultimate-list-of-security-links/">Ultimate List of Security Links</a></li>
</ul>


<p>The conference ended with a good discussion between Troy and Scott on how everything is Cyber broken. It touches upon the value of Extended Validation (EV) Certificate and how CA&rsquo;s are trying to push for them while browsers are more and more going away from them. It also touches on various proponents of HTTP and the wrong messages that are getting spread to a broader audience and also about <a href="https://scotthelme.co.uk/revocation-is-broken/">certificate revocations</a> and a lot more. It was a fun discussion and a great end to the three-day event.</p>

<h3>Location and Food</h3>

<p>NDC Security was held at <a href="https://www.qthotelsandresorts.com/gold-coast/">QT Gold Coast, Queensland</a> and well organized. Coffee and drinks were available all throughout the day with a barista on the last day (which was cool). Food was served at start, breaks, and lunch and was good. The conference rooms were great and spacious and had reasonable good internet. Did not face much connectivity issues and everything ran smoothly.</p>

<p><img src="/images/ndc_security_food_location.jpg" class="center" alt="NDC Securtiy, 2018 - Food and Location"></p>

<p>One of the things I first did after coming from the conference was to <a href="https://rahulpnath.com/blog/https-for-free-and-why-you-should-care/">move this blog over to HTTPS</a>. I had been procrastinating long on this, but there were enough reasons to make a move now. Also, there are a bunch of things that catch my eye at client places and other web sites that I visit often. Attending the conference and workshop has been a great value add and recommend to anyone if you have a chance to attend that. For the others, most of the content is available in <a href="https://www.pluralsight.com/">Pluralsight</a>.</p>

<p><em>PS: Special thanks to <a href="https://rahulpnath.com/blog/finding-a-job-abroad/">Readify</a> for sending me to this conference and also providing a &lsquo;paid vacation (accommodation)&rsquo; in Gold Coast. It was a nice three-day break for my wife and son also.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up Build and Deploy Pipeline for a .NET Core Console Application]]></title>
    <link href="http://rahulpnath.com/blog/build-and-deploy-a-net-core-console-application/"/>
    <updated>2018-01-03T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/build-and-deploy-a-net-core-console-application</id>
    <content type="html"><![CDATA[<p>I was given a console application written in .NET Core 2.0 and asked to set up a continuous deployment pipeline using <a href="https://www.jetbrains.com/teamcity/">TeamCity</a> and <a href="https://octopus.com/">Octopus Deploy</a>. I struggled a bit with some parts, so thought it&rsquo;s worth putting together a post on how I went about it. If you have a better or different way of doing things, please shout out in the comments below.</p>

<p>At the end of this post, we will have a console application that is automatically deployed to a server and running, anytime a change is pushed to the associated source control repository.</p>

<h3>Setting Up TeamCity</h3>

<p>Create a <a href="https://confluence.jetbrains.com/display/TCD10/Creating+and+Editing+Projects">New Project</a> and add a <a href="https://confluence.jetbrains.com/display/TCD10/Creating+and+Editing+Build+Configurations">new build configuration</a> just like you would for any other project. Since the application is in .NET Core, install the <a href="https://github.com/JetBrains/teamcity-dotnet-plugin">.NET CLI plugin</a> on the TeamCity server.</p>

<p><img src="/images/net_core_teamcity_build_steps.png" alt="Build Steps to build .Net Core"></p>

<p>The first three build steps use the .NET CLI to <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-restore?tabs=netcore2x">Restore</a>, <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-build?tabs=netcore2x">Build</a> and <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-publish?tabs=netcore2x">Publish</a> the application. Thee three steps restore the dependencies of the project, builds it and publishes all the relevant DLL&rsquo;s into the publish folder.</p>

<p>The published application now needs to be packaged for deployment. In my case, deployments are managed using Octopus Deploy. For .NET projects, the preferred way of packaging for Octopus is using <a href="https://octopus.com/docs/packaging-applications/creating-packages/nuget-packages/using-octopack">Octopack</a>. However, OctoPack does not support .NET Core projects. The recommendation is to either use <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-pack?tabs=netcore2x">dotnet pack</a> or <a href="https://octopus.com/docs/packaging-applications/creating-packages/nuget-packages/using-octo.exe">Octo.exe pack</a>. Using the latter I have set up a Command Line build step to pack the contents of the published folder into a zip (.nupkg) file.</p>

<pre><code class="bash">octo pack --id ApplicationName --version %build.number% --basePath published-app 
</code></pre>

<p>The NuGet package is published to the NuGet server used by Octopus. Using the <a href="https://octopus.com/docs/api-and-integration/teamcity">Octopus Deploy: Create Release</a> build step, a new release is triggered in Octopus Deploy.</p>

<h3>Setting Up Octopus Deploy</h3>

<p>Create a <a href="https://octopus.com/docs/deployment-process/projects">new project</a> in Octopus Deploy to manage deployments. Under the Process tab, I have two <a href="https://octopus.com/docs/deployment-process/steps">steps</a> - one to deploy the Package and another to start the application.</p>

<p><img src="/images/net_core_octopus_deploy_process.png" alt="Octopus Deploy Process Steps"></p>

<p>For the Deploy Package step I have enabled Custom Deployment Scripts and <a href="https://octopus.com/docs/deploying-applications/deploying-asp.net-core-web-applications/json-configuration-variables-feature">JSON Configuration variables</a>. Under the pre-deployment script, I stop any existing .NET applications. If multiple .NET applications are running on the box, select your application explicitly.</p>

<pre><code class="powershell Pre Deployment Script">Stop-Process -Name dotnet -Force -ErrorAction SilentlyContinue
</code></pre>

<p>Once the package is deployed, the custom script starts up the application.</p>

<pre><code class="powershell Run App">cd C:\DeploymentFolder
Start-Process dotnet .\ApplicationName.dll
</code></pre>

<p>With all that set, any time a change is pushed into the source control repository, TeamCity picks that up, build and triggers a deployment to the configured environments in Octopus Deploy. Hope this helps!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scheduling Recurring Jobs With a Cool-Off Period]]></title>
    <link href="http://rahulpnath.com/blog/scheduling-recurring-jobs-with-a-cool-off-period/"/>
    <updated>2017-12-24T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/scheduling-recurring-jobs-with-a-cool-off-period</id>
    <content type="html"><![CDATA[<p><a href="https://flic.kr/p/8ys6Hs" class="center" ><img class="center" alt="Scheduling" src="/images\scheduling_jobs.jpg" /></a></p>

<p>At one of my clients, they had a requirement of scheduling various rules to sent our alert messages via SMS, Email, etc. A Rule consists of below and a few other properties</p>

<ul>
<li><strong>Stored Procedure</strong>: The Stored Procedure (yes you read it correctly) to check if an alert needs to be raised</li>
<li><strong>Polling Interval</strong>: The time interval in which a Rule needs to be checked.</li>
<li><strong>Cool-Off Period</strong>: Time to wait before running Rule again after an alert was raised.</li>
</ul>


<p>All Rules are stored in a database. New rules can be added and existing ones updated via an external application. Since the client is not yet in the Cloud, using any of Azure Functions, Lambda, Web Jobs, etc. are out of the question. It needs to be a service running on-premise, so I decided to keep it as a Windows service.</p>

<pre><code class="csharp"> public class Rule
{
    public int Id { get; set; }
    public string Name { get; set; }
    public string StoredProc { get; set; }
    public TimeSpan PollingInterval { get; set; }
    public TimeSpan CoolOffPeriod { get; set; }
    ...
}
</code></pre>

<p>Because of my past good experiences with <a href="https://www.hangfire.io/">HangFire</a> I initially set off using that only to discover soon that it can schedule jobs only to the minute level. Even though this is a <a href="https://github.com/HangfireIO/Hangfire/issues/167">feature that has been discussed for a long time</a>, it&rsquo;s yet to be implemented. Since some of the rules are critical to the business, they want to be notified as soon as possible. This means having a polling interval in seconds for those rules.</p>

<p>After reaching out to my <a href="http://www.rahulpnath.com/blog/finding-a-job-abroad/">friends at Readify</a>, I decided to use <a href="https://www.quartz-scheduler.net/">Quartz.net</a>. Many had good experiences using it in the past and recommended it highly. One another option that came up was <a href="https://github.com/fluentscheduler/FluentScheduler">FluentScheduler</a>. There was no particular reason to go with Quartz.net.</p>

<blockquote><p><em>Quartz.NET is a full-featured, open source job scheduling system that can be used from smallest apps to large-scale enterprise systems.</em></p></blockquote>

<p>Setting up and getting started with Quartz scheduler is fast and easy. The library has a <a href="https://www.quartz-scheduler.net/documentation/index.html">well-written documentation</a>. You can update the applications configuration file to tweak various attributes of the scheduler.</p>

<pre><code class="xml App/Web.config file">&lt;configuration&gt;
  &lt;configSections&gt;
    &lt;section name="quartz" type="System.Configuration.NameValueSectionHandler, System, Version=1.0.5000.0,Culture=neutral, PublicKeyToken=b77a5c561934e089" /&gt;
  &lt;/configSections&gt;
  &lt;quartz&gt;
    &lt;add key="quartz.scheduler.instanceName" value="TestScheduler" /&gt;
    &lt;add key="quartz.jobStore.type" value="Quartz.Simpl.RAMJobStore, Quartz" /&gt;
  &lt;/quartz&gt;
&lt;/configuration&gt;
</code></pre>

<p>The <a href="http://www.quartz-scheduler.org/api/2.2.1/org/quartz/simpl/RAMJobStore.html">RAMJobStore</a> indicates the store to use for storing job. There are other job stores available if you want persistence of jobs anytime the application restarts.</p>

<h3>Setting Up Jobs</h3>

<p>Basically, there are three jobs - Alert Job, CoolOff Job, and Refresh Job - set up for the whole application. The Alert and Refresh Jobs are scheduled on application start. The CoolOff Job is triggered by the Alert Job as required. Any data that is required by the job is passed in using <a href="https://www.quartz-scheduler.net/documentation/quartz-2.x/tutorial/more-about-jobs.html#jobdatamap">JobDataMap</a>.</p>

<pre><code class="csharp Schedule an Alert Job">...
var job = JobBuilder.Create&lt;AlertJob&gt;()
    .WithIdentity(rule.GetJobKey())
    .WithDescription(rule.Name)
    .SetJobData(rule)
    .Build();

var trigger = TriggerBuilder
    .Create()
    .WithIdentity(rule.GetTriggerKey())
    .StartNow()
    .WithSimpleSchedule(a =&gt; a
        .WithIntervalInSeconds((int)rule.PollingInterval.TotalSeconds)
        .RepeatForever())
    .Build();

scheduler.ScheduleJob(job, trigger);
</code></pre>

<h4><strong>Alert Jobs</strong></h4>

<p>The Alert Job is responsible for checking the stored procedure and sending the alerts if required. If an alert is sent, it starts the CoolOff Job and pauses the current job instance. THe DisallowConcurrentExecution prevents multiple instances of the Job having the same key does not execute concurrently. We explicitly set the Job Key based on the Rule Id. This prevents any duplicate messages getting sent out if any of the job instances takes more time to execute than its set polling interval.</p>

<pre><code class="csharp">[DisallowConcurrentExecution]
public class AlertJob : Job
{
    public void Execute(IJobExecutionContext context)
    {
        var alert = context.GetRuleFromJobData();
        var message = GetAlertMessage(alert);
        if(message != null)
        {
            SendMessage(message);
            CoolOff(alert);
        }    
    }

    public void CoolOff(Rule rule)
    {
        var job = JobBuilder.Create&lt;CoolOffJob&gt;()
            .WithIdentity(jobKey)
            .WithDescription(rule.MessageTitle)
            .SetJobData(rule)
            .Build();

        var trigger = TriggerBuilder
            .Create()
            .WithIdentity(rule.GetCoolOffTriggerKey())
            .StartAt(rule.GetCoolOffDateTimeOffset())
            .Build();

        scheduler.PauseJob(rule.GetJobKey());
        scheduler.ScheduleJob(job, trigger);
    }
    ...
}
</code></pre>

<h4><strong>Cool-Off Job</strong></h4>

<p>Cool-Off Jobs is a one time job scheduled by the Alert Job after an alert is sent successfully. The CoolOff job is scheduled to start after the Cool-Off time as configured for the alert. This triggers the job only after the set amount of time. It Resumes the original Rule Job to continue execution.</p>

<pre><code class="csharp">public class CoolOffJob : IJob
{
    public void Execute(IJobExecutionContext context)
    {
        var alert = context.GetRuleFromJobData();
        ScheduleHelper.ResumeJob(alert);
    }
}
</code></pre>

<h4><strong>Refresh Job</strong></h4>

<p>The Refresh Job is a recurring job, that polls the database for any changes to the Rules themselves If any change is detected,it removes the existing schedules for the alert and adds the updated alert job.</p>

<pre><code class="csharp">[DisallowConcurrentExecution]
public class RefreshJob : IJob
{
    public void Execute(IJobExecutionContext context)
    {
        var allRules = GetAllRules();
        ScheduleHelper.RefreshRules(allRules);
    }
}
</code></pre>

<p>With these three jobs, all the rules get scheduled at the start of the application and run continuously. Anytime a change is made to the rule itself, the Refresh Job refreshes it within the time interval that it is scheduled for.</p>

<div class="alert alert-info">
<b>Tip:</b>If there are a lot of rules with the same polling interval it will be good to stagger their starting time using a delayed start per job instance. Doing that will make sure that all jobs do not get polled for at the same time.
</div>


<p>So far I have found the Quartz library stable and reliable and have not faced any issues with it. The library is also quite flexible and adapts well to the different needs.</p>

<p>Hope this helps. Merry Xmas!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Generating a Large PDF from Website Contents - Merging PDF Files]]></title>
    <link href="http://rahulpnath.com/blog/generating-a-large-pdf-from-website-contents-part-iii/"/>
    <updated>2017-09-19T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/generating-a-large-pdf-from-website-contents-part-iii</id>
    <content type="html"><![CDATA[<p>In the previous post, <a href="http://www.rahulpnath.com/blog/generating-a-large-pdf-from-website-contents-part-ii/">Generating a Large PDF from Website Contents - HTML to PDF, Bookmarks and Handling Empty Pages</a>, we saw how to generate a PDF from HTML and add bookmarks to the generated PDF files. The PDF file generated is for an individual section which now needs to be merged to form a single PDF file. The individual PDF files contain the relevant content for the section and related bookmarks, which needs to be combined into a single PDF file.</p>

<p>One of the important things to keep intact when merging is the document hierarchy. The Sections, Sub-Categories, and Categories should align correctly so that the final bookmark tree and the Table of Contents appear correctly. It is best to maintain the list of individual PDF document streams in the same hierarchy as required. Since we know the required structure right from the UI, this can be easily achieved by using a data structure similar as shown below</p>

<pre><code class="csharp">public class DocumentSection
{
    public MemoryStream PDFDocument {get; set;}

    public List&lt;DocumentSection&gt; ChildSections {get; set;}

    ... // Any additional details that you need
}
</code></pre>

<p>The above structure allows us to maintain a tree-like structure of the document. The structure is the same as that is provided to the user to select the PDF options. I used the <a href="https://www.nuget.org/packages/iTextSharp-LGPL/">iTextSharp</a> library to merge PDF documents. To interact with the PDF, we first need to create a PdfReader object from the stream. Using the  SimpleBookmark class, we can get the existing bookmarks for the PDF.</p>

<pre><code class="csharp">var pdfReader = new PdfReader(stream);
ArrayList bookmarks = SimpleBookmark.GetBookmark(pdfReader);
</code></pre>

<p>iText representation of bookmarks is a bit complex. It represents them as an ArrayList of Hashtables. The Hashtable has keys like Action, Title, Page, Kids, etc. Kids property represents child bookmarks and is the same ArrayList type. Since it was hard to work with this structure, I created a wrapper class to interact easily with the bookmarks.</p>

<pre><code class="csharp">public class Bookmark
{
    public Bookmark(
        string title, string destinationType, int pageNumber, 
        float xLeft, float yTop, float zZoom)
    {
        Children = new List&lt;Bookmark&gt;();
        Title = title;
        PageNumber = pageNumber;
        DestinationType = destinationType ?? "XYZ";
        XLeft = xLeft;
        YTop = yTop;
        ZZoom = zZoom;
        PageBreak = false;
    }

    ... // Class properties for the constructor parameters

    public ArrayList ToiTextBookmark()
    {
        ArrayList arrayList = new ArrayList
        {
            ToiTextBookmark(this),
        };
        return arrayList;
    }

    private Hashtable ToiTextBookmark(Bookmark bookmark)
    {
        var kids = new ArrayList();
        var hashTable = new Hashtable
        {
            ["Action"] = "GoTo",
            ["Title"] = bookmark.Title,
            ["Page"] = $@"{bookmark.PageNumber} {bookmark.DestinationType} 
                         {bookmark.XLeft} {bookmark.YTop} {bookmark.ZZoom}",
            ["Kids"] = kids,
        };

        foreach (var childBookmark in bookmark.Children)
        {
            kids.Add(ToiTextBookmark(childBookmark));
        }

        return hashTable;
    }
}
</code></pre>

<p>Recursively iterating through the list of DocumentSections, I add all the bookmarks to a root Bookmark class. The root bookmark class represents the full bookmark of the PDF file. The PageNumber is offset using a counter variable. The counter variable is incremented by the number of pages in each of PDF section (<em>pdfReader.NumberOfPages</em>) as it gets merged to the bookmark root. This ensures that the bookmark points to the correct bookmark page in the combined PDF file.</p>

<p>The individual documents are then merged by iterating through all the generated document sections. Once done we get the final PDF as a byte array which is returned to the user.</p>

<pre><code class="csharp">public byte[] MergeSections(List&lt;DocumentSection&gt; documentSections, Bookmark bookmarkRoot)
{
    int pageNumber = 0;
    using (var stream = new MemoryStream())
    {
        var document = new Document();
        var pdfWriter = PdfWriter.GetInstance(document, stream);
        document.Open();
        var pdfContent = pdfWriter.DirectContent;
        MergeSectionIntoDocument(documentSections, document, pdfContent, pdfWriter, pageNumber);
        pdfWriter.Outlines = bookmarkRoot.ToiTextBookmark();
        document.Close();
        stream.Flush();
        return stream.ToArray();
    }
}

private void MergeSectionIntoDocument(
    List&lt;DocumentSection&gt; documentSections,
    Document document,
    PdfContentByte pdfContent,
    PdfWriter pdfWriter,
    int pageNumber)
{
    foreach (var documentSection in documentSections)
    {
        var stream = documentSection.DocumentStream;
        stream.Position = 0;
        var pdfReader = new PdfReader(stream);

        for (var i = 1; i &lt;= pdfReader.NumberOfPages; i++)
        {
            var page = pdfWriter.GetImportedPage(pdfReader, i);
            document.SetPageSize(new iTextSharp.text.Rectangle(0.0F, 0.0F, page.Width, page.Height));
            document.NewPage();
            pageNumber++;
            pdfContent.AddTemplate(page, 0, 0);
            this.AddPageNumber(pdfContent, document, pageNumber);
        }

        if(documentSection.ChildSections.Any())
            MergeSectionIntoDocument(documentSection.ChildSections, document, pdfContent, pdfWriter, pageNumber);
    }
}
</code></pre>

<p>To generate a Table of Contents (ToC), we can use the root bookmark information. We need to manually create a PDF page, read the bookmark text and add links to the page with the required font and styling. iText provides API&rsquo;s to create custom PDF pages.</p>

<p>We are now able to generate a single PDF based on the website contents.</p>
]]></content>
  </entry>
  
</feed>
