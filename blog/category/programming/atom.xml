<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/programming/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2016-09-20T04:50:46+00:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Keeping Sensitive Configuration Data Out of Source Control]]></title>
    <link href="http://rahulpnath.com/blog/keeping-sensitive-configuration-data-out-of-source-control/"/>
    <updated>2016-09-19T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/keeping-sensitive-configuration-data-out-of-source-control</id>
    <content type="html"><![CDATA[<p>Most applications today deals with some form of sensitive information. The most commonly seen are database connection strings, API keys, token etc. The web.config seems the best place to have these values, but it definitely is not. In most cases it gets pushed into the source control systems as well. If it is a private repository then you at least have one level of security on top of it. It still exposes sensitive information to anyone who has access to the repository. It’s worse when the <a href="http://www.internetnews.com/blog/skerner/github-search-exposes-passwords.html">repository is public</a>.</p>

<p><img alt="Keep sensitive data out of source control" src="/images/sensitivedata_source_control.png" /></p>

<p>There are different ways you can  avoid pushing sensitive data into source control. In this post, I will explore options that I am familiar with.</p>

<blockquote><p><em>Use configuration files as template definitions for the configuration data your application requires. Have the actual values stored elsewhere</em></p></blockquote>

<h3>Azure App Settings</h3>

<p>If you are deploying your application as a Web App on Azure, you can store <a href="https://azure.microsoft.com/en-us/blog/windows-azure-web-sites-how-application-strings-and-connection-strings-work/">application settings and connection strings in Azure</a>. At runtime, Windows Azure Web Sites automatically retrieves these values for you and makes them available to code running in your website. This removes the need for having sensitive data in the configuration file.</p>

<p><img alt="Azure App Settings and Connection Strings" src="/images/sensitiveData_azure_app_settings.png" /></p>

<h3>Release Management Tools</h3>

<p>Release management tools like Octopus Deploy, Microsoft Release Management, that performs configuration transformation. It supports creating different environments (development, production) and corresponding configurations. On creating a package for an environment, it applies the corresponding environment configurations</p>

<p><img alt="Release Management Tools - Octopus Deploy" src="/images/sensitiveData_releaseManagement_tool_octopus.png" /></p>

<p>Packaging embeds the configuration value into the configuration file. This makes it available to anyone who has access to the host systems.</p>

<h3>Azure Key Vault</h3>

<p>Azure Key Vault acts as a centralized repository for all sensitive information. Key vault stores cryptographic keys and Secrets and makes them available over a HTTP Api. The objects (keys and secrets) in key vault has unique identifier to retrieve them. Check <a href="http://www.rahulpnath.com/blog/azure-key-vault-in-a-real-world-application/">Azure Key Vault in real world application</a> for more details on how to achieve this. A client application can <a href="http://www.rahulpnath.com/blog/authenticating-a-client-application-with-azure-key-vault/">authenticate with Azure Key Vault using a ClientID/secret or ClientID/certificate</a>. Using certificate to authenticate is the preferred approach. To get Keys/Secret from key vault all you need is the AD Application Id, the client secret or certificate identifier and the key/secret names. The certificate itself can be deployed separately into the application host.</p>

<pre><code class="XML">&lt;appSettings&gt;
  &lt;add key="KeyVaultUrl" value="https://testvaultrahul.vault.azure.net"/&gt;
  &lt;add key="ADApplicationId" value="" /&gt;
  &lt;add key="ADCertificateThumbprint" value="" /&gt;
  &lt;add key="DbConnectionString" value="SqlConnectionString"/&gt;
  &lt;add key ="ApiToken" value="ApiToken/cfedea84815e4ca8bc19cf8eb943ee13"/&gt;
&lt;/appSettings&gt;
</code></pre>

<p>If you are using the &lsquo;client secret&rsquo; to authenticate then the configuration file will have the Secret. In either cases, you should follow either of the previous approaches to keep the Application Id and authentication information out of configuration. The advantage of using <a href="http://www.rahulpnath.com/blog/category/azure-key-vault/">Key Vault</a> is that it is a centralized  repository for all your sensitive data, across different applications. You can also restrict access permissions per application.</p>

<p>These are some approaches to keep sensitive information out of source control. What approach do you use? Irrespective of the approach you use, make sure that you don’t check them in!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Hotfix : Things to Remember]]></title>
    <link href="http://rahulpnath.com/blog/data_hotfix/"/>
    <updated>2016-08-25T04:27:32+00:00</updated>
    <id>http://rahulpnath.com/blog/data_hotfix</id>
    <content type="html"><![CDATA[<p>Yesterday I was late to leave office as I had to data fix some of the systems that we are currently building. We had just migrated few hundred clients onto the new platform. Invoices generated for the clients had wrong invoice amounts due to some mismatching data used when migrating. We had the expected invoice from the old system which made finding the problem easy. We ran a few scripts to correct the data in different systems and fixed the issue.</p>

<p><a href="http://static1.squarespace.com/static/54652521e4b0045935420a6c/t/548dee03e4b0f1b25cb560d5/1418587652009/Data.jpg?format=1500w">
    <img class="center" alt="Data Hotfix" src="/images/data_hotfix.jpg" />
</a></p>

<div class="alert alert-warning">
<strong>WARNING!</strong> Normally I do not recommend making any changes directly in production server. In this case, there was a business urgency and was forced to do the data fix the same night, for smooth functioning the day after. We still managed to get in some testing in the development environment before running it in production.
</div>


<h3>It All Starts with a Few</h3>

<p>I have seen it repeatedly happen that this kind of data fixes starts with a few in the beginning. Within a short span of time the affected data size grows drastically and manual updates might not be a good solution.</p>

<blockquote><p><em>If you get a second thought of whether to script the fix or not, then you should script it.</em></p></blockquote>

<p> Yesterday it started with data fix for 30 clients and the fix was relatively small. It could either be through UI or API. Fix through the UI took around 45 seconds each, and there were two of us. So it was just a matter of 12-15 minutes to fix it. While fixing, one of us found an extra scenario where the same fix needs to be applied. Re-running the query to find such clients bombarded the number to 379. At this moment, I stood up and said <em>I am going to script this. There is no way  I am doing this manually.</em> Manually fixing this would take five man hours, but will finish in two and half hours, as there were two of us. Even writing the script is going to take around an hour but that&rsquo;s just one man hour.</p>

<blockquote><p><em>There is happiness you get when you script the fix and not manually plow through the UI fixing each of them</em></p></blockquote>

<p>The script was in C#, written as a test case, invoked from a test runner (which I don&rsquo;t feel great about now) updating the systems with the data fix. It did its job and fixed all the cases it was supposed to. But I was not happy with the approach that I had chosen to make the fix. Correcting production data through a unit test script does not sound a robust solution. The reason to choose tests was that the test project had all the code required to access the other systems. It was just about changing the configuration values to point to the production system. It was the shortest path to having at least one client updated and verified.</p>

<p>Having it as a test script restricted me from scaling the update process (though I could have done some fancy things to <a href="https://xunit.github.io/docs/running-tests-in-parallel.html">run tests in parallel</a>). It also forced me to hard-code the input data.Logging was harder  and I used <a href="https://msdn.microsoft.com/en-us/library/system.diagnostics.debug.writeline(v=vs.110).aspx">Debug.WriteLine</a> to the VS output window. All those were the aftermath of choosing the wrong execution method - running it as a test script!</p>

<p>In retrospective, here are a few things that I should have done different and should be doing if ever I am in a similar situation again.</p>

<h4><strong>Create Stand-alone Executable</strong></h4>

<p>Having a stand-alone executable running the script provides the capability to scale the number of processes as I wanted. Input can be passed as a file or as an argument to the application allowing to break the large data set into smaller subsets.</p>

<h4><strong>Log Error and Success</strong></h4>

<p>It&rsquo;s very much possible that the &lsquo;fix-to-fix errors&rsquo; can go wrong or throw exceptions. So handle for errors and log appropriate message to take any corrective actions. It&rsquo;s better to log to a file or other durable storage as that is more foolproof. Logging to the output window (Debug.Writeline/Console.Writeline) is not recommended, as there is a risk of accidentally losing it (with another test run or closing VS).</p>

<p> Logging successes are equally important to keep track of fixed records. It helps in cases where the process terminates suddenly while processing a set of data. It gives a track of all data sets that were successfully processed and exclude from following runs.</p>

<h4><strong>Test</strong></h4>

<p>It is very likely that the script has bugs and does not handle all possible cases. So as with any code, testing the data fix script is also mandatory. Preferably, test in a development/test environment, if not try for a small subset of input in the production. In my case, I was able to test in the development environment and then in production. But still, I ran a small subset in production first and ended up finding an issue that I could not find in development.</p>

<h4><strong>Parallelize if Possible</strong></h4>

<p>In cases where the data fixes are independent of each other (which likely is when dealing with large data fixes), each of the updates can be in parallel. Also using nonblocking calls when updating across the network helps speed up the process, by reducing the idle time and improves the overall processing time.</p>

<h4><strong>Parameterize Input</strong></h4>

<p>Parameterizing of input to the script (console) application helps when you want to scale the application. In my case updating each of the clients took around 8-10 seconds as it involved calling multiple geographically distributed systems. (Updating a system in the US from Australia does take a while!). Having a parameterized application enables to have multiple applications running with different input sets updating the data and speeds up the overall processing time.</p>

<p>It&rsquo;s hard to come up with a solid plan for critical data fixes. It might not be possible to follow all of the points above. Also, there might be a lot other things to be done other than these. These are just a few things for reference so that I can stop, take a look and move on when a similar need arises.  Hope this helps someone else too! Drop in a comment if you have any tips for the &lsquo;eleventh hour&rsquo; fix!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NDC Sydney]]></title>
    <link href="http://rahulpnath.com/blog/ndc-sydney/"/>
    <updated>2016-08-08T15:26:15+00:00</updated>
    <id>http://rahulpnath.com/blog/ndc-sydney</id>
    <content type="html"><![CDATA[<p>It was a busy week with <a href="http://ndcsydney.com/">NDC Sydney</a> and a lot of other user group conferences happening at the same time since all the international speakers were in town.The conference was three days long with 105 speakers, 37 technologies, and 137 talks. Some the popular speakers were <a href="http://www.hanselman.com/">Scott Hanselman</a>, <a href="http://stackoverflow.com/users/22656/jon-skeet">Jon Skeet</a>, <a href="http://blog.ploeh.dk/">Mark Seemann</a>, <a href="http://odetocode.com/about/scott-allen">Scott Allen</a>,<a href="https://www.troyhunt.com/">Troy Hunt</a> and <a href="http://ndcsydney.com/speakers/">a lot more</a>.</p>

<p><img class="center" alt="NDC Sydney" src="/images/ndc_logo.png" /></p>

<h3>Sessions</h3>

<p>Each talk is one hour long and <a href="http://ndcsydney.com/agenda/">seven talks happen at the same time</a>. The talks that I attended are:</p>

<ul>
<li><a href="http://ndcsydney.com/talk/keynote/">Keynote: “If I knew then what I know now…” – Teaching Tomorrow’s Web to Yesterday’s Programmer</a></li>
<li><a href="http://ndcsydney.com/talk/stairway-to-cloud-orleans-framework-for-building-halo-scale-systems/">Stairway to Cloud: Orleans Framework for building Halo-scale systems</a></li>
<li><a href="http://ndcsydney.com/talk/50-shades-of-appsec/">50 Shades of AppSec</a></li>
<li><a href="http://ndcsydney.com/talk/adventures-in-building-a-fast-web-server-for-asp-net-core-1-0/">ASP.NET Core Kestrel: Adventures in building a fast web server</a></li>
<li><a href="http://ndcsydney.com/talk/domain-architecture-isomorphism-and-the-inverse-conway-maneuver/">Domain Architecture Isomorphism and the Inverse Conway Maneuver</a></li>
<li>Left early from <a href="http://ndcsydney.com/talk/building-solid-asp-net-core-1-0-apps/">Building SOLID ASP.NET Core 1.0 Apps</a> to attend <a href="http://www.meetup.com/Sydney-Alt-Net/events/231759202/">Alt.net user group</a></li>
<li><a href="http://ndcsydney.com/talk/making-hacking-childs-play/">Making Hacking Child’s Play</a></li>
<li><a href="http://ndcsydney.com/talk/understanding-an-architecture-for-the-cloud/">A cloud architecture – Azure from the bottom up</a></li>
<li><a href="http://ndcsydney.com/talk/functional-architecture-the-pits-of-success/">Functional Architecture: the Pits of Success</a></li>
<li>Moved to <a href="http://ndcsydney.com/talk/lightning-talks/">Lightning talks</a> after getting bored with <a href="http://ndcsydney.com/talk/lets-talk-auth/">Let&rsquo;s talk auth</a></li>
<li><a href="http://ndcsydney.com/talk/building-reactive-services-using-functional-programming/">Building Reactive Services using Functional Programming</a></li>
<li><a href="http://ndcsydney.com/talk/microtesting-how-we-set-fire-to-the-testing-pyramid-while-ensuring-confidence/">Microtesting: How We Set Fire To The Testing Pyramid While Ensuring Confidence</a></li>
<li><a href="http://ndcsydney.com/talk/what-does-an-open-source-microsoft-web-platform-look-like/">What does an “Open Source Microsoft Web Framework” look like</a></li>
<li><a href="http://ndcsydney.com/talk/google-cloud-platform/">Accessing the Google Cloud Platform with C#</a></li>
<li><a href="http://ndcsydney.com/talk/one-kata-three-languages/">One kata, three languages</a></li>
<li><a href="http://ndcsydney.com/talk/head-to-head/">Head to Head: Scott Allen and Jon Skeet</a></li>
<li><a href="http://ndcsydney.com/talk/deploying-and-scaling-microservices/">Deploying and Scaling Microservices</a></li>
<li><a href="http://ndcsydney.com/talk/the-experimentation-mindset/">The Experimentation Mindset</a></li>
<li><a href="http://ndcsydney.com/talk/c-7-the-future/">C# 7</a></li>
</ul>


<p>All sessions are recorded and are <a href="https://vimeo.com/ndcconferences">available here</a>. I hope the NDC Sydney ones too will be there in some time.</p>

<h3>Networking</h3>

<p>Events like this are a great place to network with other people in the industry and was one of the reasons I wanted to attend NDC. I am a regular reader of Mark Seemann&rsquo;s (@ploeh) blog, and his ideas resonate with me a lot. Also, I find his <a href="https://www.pluralsight.com/authors/mark-seemann">Pluralsight</a> videos and his book, <a href="http://amzn.to/2aFmtiC">Dependency Injection in .NET</a> helpful. It was great to meet him in person and enjoyed both of his talks on FSharp.</p>

<p><img class="center" alt="With Mark Seemann (ploeh)" src="/images/ndc_ploeh.jpg" /></p>

<h3>Sponsors</h3>

<p>Most of the <a href="http://ndcsydney.com/page/partnership/">event sponsors</a> had they stall at the conference, spreading their brand (with goodies and t-shirts) and also the work they do (a good way to attract talent to the company). There were also raffles for some big prices like Bose headphones, Das keyboards, Drones, Coffee machines, Raspberry Pi, etc. I was lucky enough to win a Raspberry Pi3 from <a href="https://twitter.com/RavenDB">@ravendb</a>.</p>

<p><img alt="Won a Raspberry Pi3. Ravendb raffle @ NDCSydney" src="/images/ndc_raspberrypi.jpg" /></p>

<p>It&rsquo;s confirmed that <a href="https://twitter.com/samnewman/status/761402917798555648">NDC Sydney is coming back next year</a>. If you are in town during that time, make sure you reserve a seat.  Look out for the early bird tickets, those are cheap, and the conference is worth it. Thanks to Readify for sponsoring my tickets and it&rsquo;s <a href="http://www.rahulpnath.com/blog/finding-a-job-abroad/">one of the good things about working with Readify</a>.</p>

<p>See you at NDC Sydney next year!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Refactoring to Improve Readability - Separating Business Language and Programming Language Semantics]]></title>
    <link href="http://rahulpnath.com/blog/refactoring-to-improve-readability-separating-business-language-and-programming-language-semantics/"/>
    <updated>2016-07-18T04:52:35+00:00</updated>
    <id>http://rahulpnath.com/blog/refactoring-to-improve-readability-separating-business-language-and-programming-language-semantics</id>
    <content type="html"><![CDATA[<p>Often we write ourselves or come across code that has both business language and the programming language semantics mixed together. This makes it very hard to reason about the code and also fix any issues. It&rsquo;s easier to read code that is composed of different smaller individual functions doing a single thing.</p>

<p>If you follow the <em>One Level of Abstraction per Function Rule</em> or the <em>Stepdown Rule</em> as mentioned in the book <a href="http://www.amazon.com/gp/product/0132350882/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0132350882&amp;linkCode=as2&amp;tag=rahulpnath-20&amp;linkId=CVCVZFAR5SBYVMJW">Clean Code</a> (I <a href="http://www.rahulpnath.com/blog/language-agnostic-books-for-every-developer-2/">recommend reading it</a> if you have not already), it is easier to keep the business and programming language semantics separate.</p>

<blockquote><p><em>We want the code to read like a top-down narrative. We want every function to be followed by those at the next level of abstraction so that we can read the program, descending one level of abstraction at a time as we read down the list of functions. Making the code read like a top-down set of TO paragraphs is an effective technique for
keeping the abstraction level consistent.</em></p></blockquote>

<p>Recently while fixing a bug in one of the applications that I am currently working on, I came across a code with the business and programming language semantics mixed together. This made it really hard to understand the code and fixing it. So I decided to refactor it a bit before fixing the bug.</p>

<p> <a href="http://www.slideshare.net/kvg452/the-art-of-readable-code-31322040">
<img class="center" src="/images/readable_code.jpg" alt="Code should be readable" />
</a></p>

<p>The application is a subscription based service for renting books, videos, games etc. and enabled customers to have different subscription plans and terms. Currently, we are migrating away from the custom built billing module that the application uses to a SAAS based billing provider to make invoicing and billing easy and manageable. In code, a <em>Subscription</em> holds a list of <em>SubscriptionTerm</em> items, that specifies the different terms that a customer has for the specific subscription. A term typically has a start date, an optional end date and a price for that specific term. A null end date indicates that the subscription term is valid throughout the customer lifetime in the system.</p>

<pre><code class="csharp">public class Subscription
{
    public List&lt;SubscriptionTerm&gt; Terms { get; set; }
}

public class SubscriptionTerm
{
    public int Id { get; set; }
    public double Price { get; set; }
    public DateTime StartDate { get; set; }
    public DateTime? EndDate { get; set; }
}
</code></pre>

<p>But in the new system to which we are migrating to, does not support subscription terms that overlap each other with a different price. This had to be data fixed manually in the source system, so we decided to perform a validation step before the actual migration. The code below does exactly that and was working fine until we started seeing that for cases where there were more than one SubscriptionTerm without an end date and also when end date of one was the start date of another, there were no validation errors shown.</p>

<pre><code class="csharp">public bool Validate(Subscription subscription)
{
    var hasOverlappingItems = false;
    foreach (var term in subscription.Terms)
    {
        var otherTerms = subscription.Terms.Where(a =&gt; a.Price != term.Price);
        if (otherTerms.Any())
        {
            if (
                (!term.EndDate.HasValue &amp;&amp; otherTerms.Any(a =&gt; term.StartDate &lt; a.EndDate)) ||
                (otherTerms.Where(a =&gt; !a.EndDate.HasValue).Any(a =&gt; a.StartDate &lt; term.EndDate)) ||
                (otherTerms.Any(a =&gt; term.StartDate &lt;= a.EndDate &amp;&amp; a.StartDate &lt;= term.EndDate))
            )
            {
                hasOverlappingItems = true;
                break;
            }
        }
    }

    return hasOverlappingItems;
}
</code></pre>

<p>The code, as you can see is not that readable and difficult to understand, which increases the chances of me breaking something else while trying to fix it. There were no tests covering this validator, which made it even harder to change it. While the algorithm itself to find overlappings can be improved (maybe a topic for another blog post), we will look into how we can refactor this existing code to improve its readability.</p>

<blockquote><p><em>Code is read more than written, so it&rsquo;s much better to have code optimized for reading</em></p></blockquote>

<h3>Creating the Safety Net</h3>

<p>The first logical thing to do in this case is to protect us with test cases so that any changes made does not break existing functionality.  I came up with the below test cases (<em>test data shown does not cover all cases</em>), to cover the different possible cases that this method can take.</p>

<pre><code class="csharp">[InlineData("10-Jan-2016", "10-Feb-2016", 1, "11-Feb-2016", "10-Dec-2016", 2, false)]
[InlineData("10-Jan-2015", "10-Feb-2015", 1, "20-Jan-2015", "1-Feb-2016", 2, true)]
public void ValidateReturnsExpected(
    string startDate1, string endDate1, double price1,
    string startDate2, string endDate2, double price2,
    bool expected )
{
    // Fixture setup
    var subscription = new Subscription();
    var term1 = createTerm(startDate1, endDate1, price1);
    var term2 = createTerm(startDate2, endDate2, price2);
    subscription.Terms.Add(term1);
    subscription.Terms.Add(term2);
    // Exercise system
    var sut = new OverlappingSubscriptionTermWithConflictingPriceValidator();
    var actual = sut.Validate(subscription);
    // Verify outcome
    Assert.Equal(expected, actual);
    // Teardown
}
</code></pre>

<p>All tests pass, except for those where there were issues in the destination system and I was about to fix.</p>

<h3>Refactoring for Readability</h3>

<p>Now that I have some tests to back me up for the changes that I am to make, I feel more confident to do the refactoring. Looking at the original validator code, all I see is <strong>DATETIME</strong> - There is a lot of manipulation of dates that is happening, which strongly indicates there is some abstraction waiting to be pulled out. We had seen in, <a href="http://www.rahulpnath.com/blog/thinking-beyond-primitive-values-value-objects/">Thinking Beyond Primitive Values: Value Objects</a>, that any time we use a primitive type, we should think more about the choice of type. We saw that properties that co-exist (like DateRange) should be pulled apart as Value Objects. The StartDate and EndDate in SubscriptionTerm fall exactly into that category.</p>

<pre><code class="csharp">public class DateRange
{
    public DateTime StartDate { get; private set; }

    public DateTime? EndDate { get; private set; }

    public DateRange(DateTime startDate, DateTime? endDate)
    {
        if (endDate.HasValue &amp;&amp; endDate.Value &lt; startDate)
            throw new ArgumentException("End date cannot be less than start Date");

        StartDate = startDate;
        EndDate = endDate;
    }
}
</code></pre>

<p>Since these properties are used in a lot of other places, I did not want to make a breaking change, by deleting the existing properties and adding in a new DateRange class. So I chose to add a new read-only property <em>TermPeriod</em> to SubscriptionTerm which returns a DateRange, constructed from it&rsquo;s Start and End dates, as shown below.</p>

<pre><code class="csharp">public DateRange TermPeriod
{
    get
    {
        return new DateRange(StartDate, EndDate);
    }
}
</code></pre>

<p>From the existing validator code, what we are essentially trying to check is if there are any SubscriptionTerms for a subscription that overlaps, i.e if one TermPeriod falls in the range of another. Introducing a method, <em>IsOverlapping</em> on DateRange to check if it overlaps with another DateRange seems logical at this stages. Adding a few tests cases to protect myself here to implement the IsOverlapping method in DateRange class. I also added in the tests to cover the failure scenarios that were seen before.</p>

<pre><code class="csharp Tests for IsOverlapping">[InlineData("10-Jan-2016", "10-Feb-2016", "11-Feb-2016", "10-Dec-2016", false)]
[InlineData("10-Jan-2015", "10-Feb-2015", "20-Jan-2015", "1-Feb-2016", true)]
[InlineData("10-Jan-2015", null, "20-Jan-2016", null,  true)]
[InlineData("28-Jan-16", "10-Mar-16", "10-Mar-16", null, true)]
public void OverlappingDatesReturnsExpected(
    string startDateTime1,
    string endDateTime1,
    string startDateTime2,
    string endDateTime2,
    bool expected)
{
    // Fixture setup
    var range1 = CreateDateRange(startDateTime1, endDateTime1);
    var range2 = CreateDateRange(startDateTime2, endDateTime2);
    // Exercise system
    var actual = range1.IsOverlapping(range2);
    // Verify outcome
    Assert.Equal(expected, actual);
    // Teardown
}
</code></pre>

<pre><code class="csharp IsOverlapping in DateRange">public bool IsOverlapping(DateRange dateRange)
{
    if (!EndDate.HasValue &amp;&amp; !dateRange.EndDate.HasValue)
        return true;

    if (!EndDate.HasValue)
        return StartDate &lt;= dateRange.EndDate;

    if (!dateRange.EndDate.HasValue)
        return dateRange.StartDate &lt;= EndDate;

    return StartDate &lt;= dateRange.EndDate
        &amp;&amp; dateRange.StartDate &lt;= EndDate;
}
</code></pre>

<p>Given two DateRange&rsquo;s I can now tell if they overlap or not, which now can be used to check if two SubscriptionTerms overlap. I just need to check if their TermPeriod&rsquo;s overlap. The validator code is now much more easy to understand.</p>

<pre><code class="csharp IsOverlapping in SubscriptionTerm">public bool IsOverlapping(SubscriptionTerm term)
{
    return TermPeriod.IsOverlapping(term.TermPeriod);
}
</code></pre>

<pre><code class="csharp Validator after Refactoring">public bool Validate(Subscription subscription)
{
    foreach (var term in subscription.Terms)
    {
        var termsWithDifferentPrice = subscription.Terms.Where(a =&gt; a.Price != term.Price);
        return termsWithDifferentPrice
            .Any(a =&gt; a.IsOverlapping(term));
    }

    return false;
}
</code></pre>

<p>The code now reads as a set of TO Paragraphs as mentioned in the book <a href="http://www.amazon.com/gp/product/0132350882/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0132350882&amp;linkCode=as2&amp;tag=rahulpnath-20&amp;linkId=CVCVZFAR5SBYVMJW">Clean Code</a>.</p>

<blockquote><p><em>To check if a subscription is valid, check if the subscription has overlapping SubscriptionTerms with a conflicting price. To check if two subscriptions are overlapping, check if their subscription term periods overlap each other. To check if two term periods overlap check if start date of one is before the end date of other</em></p></blockquote>

<p>Readability of code is an important aspect and should be something that we strive towards for. The above just illustrates an example of why readability of code is important and how it helps us on a longer run. It makes maintaining code really easy. Following some basic guidelines like One Level of Abstraction per Function, allows us to write more readable code. Separating code into different small readable functions covers just one aspect of Readability, there are a lot of other practices mentioned in the book <a href="http://shop.oreilly.com/product/9780596802301.do">The Art of Readable Code</a>. The sample code with all the tests and validator is available <a href="https://github.com/rahulpnath/Blog/tree/master/Refactoring/RefactoringForReadability">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Protect Yourself Against Line Ending Issues when Using Environment.Newline to Split Text]]></title>
    <link href="http://rahulpnath.com/blog/protect-yourself-against-line-ending-issues-when-using-environment-dot-newline-to-split-text/"/>
    <updated>2016-07-11T05:45:31+00:00</updated>
    <id>http://rahulpnath.com/blog/protect-yourself-against-line-ending-issues-when-using-environment-dot-newline-to-split-text</id>
    <content type="html"><![CDATA[<blockquote><p><em>In computing, a <a href="https://en.wikipedia.org/wiki/Newline">newline</a>, also known as a line ending, end of line (EOL), or line break, is a special character or sequence of characters signifying the end of a line of text and the start of a new line. The actual codes representing a newline vary across operating systems, which can be a problem when exchanging text files between systems with different newline representations.</em></p></blockquote>

<p>I was using a Resource (resx) file to store large text of comma separated values (CSV). This key-value mapping represented the mapping of product codes between an old and new system. In code, I split this whole text using <a href="https://msdn.microsoft.com/en-us/library/system.environment.newline(v=vs.110).aspx">Environment.NewLine</a> and then by comma to generate the map, as shown below.</p>

<pre><code class="csharp">AllMappings = Resources.UsageMap
    .Split(new string[] { Environment.NewLine }, StringSplitOptions.RemoveEmptyEntries)
    .Select(s =&gt; s.Split(new[] { ',' }))
    .ToDictionary(item =&gt; item[0], item =&gt; item[1]);
</code></pre>

<p>It all worked fine on my machine and even on other team members machines. There was no reason to doubt this piece of code, until on the development environment we noticed the mapped value in the destination system always null.</p>

<h3>Analyzing the Issue</h3>

<p>Since in the destination system, all the other values were getting populated as expected, except for this mapping it was easy to narrow down to the class that returned the mapping value, to be the problematic one. Initially, I thought this was an issue with the resource file not getting bundled properly. I used <a href="https://www.jetbrains.com/decompiler/">dotPeek</a> to decompile the application and verified that resource file was getting bundled properly and had exactly the same text (visually) as expected.</p>

<p><img src="/images/newline_dotpeek.png" alt ="Resource file disassembled in dotPeek" /></p>

<p>I copied the resource file text from disassembled code in dotPeek into <a href="http://www.flos-freeware.ch/notepad2.html">Notepad2</a> (configured to show the line endings) and everything started falling into place. The resource text file from the build generated code ended with LF (\n), while the one on our development machines had CRLF (\r\n). All machines, including the build machines are running Windows and the expected value for <a href="https://msdn.microsoft.com/en-us/library/system.environment.newline(v=vs.110).aspx">Environemnt.Newline</a> is CRLF - <strong> A string containing &ldquo;\r\n&rdquo; for non-Unix platforms, or a string containing &ldquo;\n&rdquo; for Unix platforms.</strong></p>

<p><figure>
<img src="/images/newline_diff.png" alt ="Difference between build generated and development machine resource file" />
<figcaption><em>Difference between build generated and development machine resource file</em></figcaption>
</figure></p>

<h3>Finding the Root Cause</h3>

<p>We use git for our source control and <a href="https://help.github.com/articles/dealing-with-line-endings/">configured to use &lsquo;auto&rsquo; line endings</a> at the repository level. This ensures that the source code, when checked out, matches the line ending format of the machine. We use <a href="https://www.atlassian.com/software/bamboo">Bamboo</a> on our build servers running Windows. The checked out files on the build server had LF line endings, which in turn gets compiled into the assembly.</p>

<p>The checkout step in Bamboo used the built in git plugin (JGit) and has certain limitations. It&rsquo;s recommended to use native git to use the full git features. JGit also has a known issue with <a href="https://jira.atlassian.com/plugins/servlet/mobile#issue/BAM-9591">line endings on a Windows machine</a> and checks out a file with LF endings. So whenever the source code was checked out, it replaced all line endings in the file with LF before compilation. So the resource file ended up having LF line endings in the assembly, and the code could no longer find Environment.Newline (\r\n) to split.</p>

<h3>Possible Fixes</h3>

<p>Two possible ways to fix this issue is</p>

<ul>
<li>Switch to using native git on the bamboo build process</li>
<li>Use LF to split the text and trim any excess characters. This reduces dependency on line endings variations and settings between different machines only until we are on a different machine which has a different format.</li>
</ul>


<p>I chose to use LF to split the text and trim any additional characters, while also <a href="https://confluence.atlassian.com/bamboo/defining-a-new-executable-capability-289277164.html">updating Bamboo to use native git</a> for checkout.</p>

<pre><code class="csharp">AllMappings = Resources.UsageMap
    .Split(new string[] {"\n"}, StringSplitOptions.RemoveEmptyEntries)
    .Select(s =&gt; s.Split(new[] { ',' }))
    .ToDictionary(item =&gt; item[0].Trim().ToUpper(), item =&gt; item[1].Trim());
</code></pre>

<h3>Protecting Against Line Endings</h3>

<p>The easiest and fastest way that this would have come to my notice was to have a unit test in place. This would ensure that the test fails on the build machine. A test like below will pass on my local but not on the build machine as UsageMap would not return any value for the destination system.</p>

<pre><code class="csharp">[Theory]
[InlineData("MovieWeek", "Weekly-Movie")]
[InlineData("Dell15", "Laptop-Group3")]
public void SutReturnsExpected(string sourceSystemCode, string expected)
{
    var sut = new UsageMap();
    var actual = sut.GetDestinationCode(sourceSystemCode);
    Assert.Equal(expected, actual);
}
</code></pre>

<p>Since there are different systems with different line endings and also applications with different line ending settings and issues of its own, there does not seem to be a &lsquo;one fix for all&rsquo; cases. The best I can think of in these cases is it protect us with such unit tests. It fails fast and brings it immediately to out notice. Have you ever had to deal with an issue with line endings and found better ways to handle them?</p>
]]></content>
  </entry>
  
</feed>
