<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | Rahul Nath]]></title>
  <link href="http://rahulpnath.com/blog/category/programming/atom.xml" rel="self"/>
  <link href="http://rahulpnath.com/"/>
  <updated>2018-12-17T02:49:54+00:00</updated>
  <id>http://rahulpnath.com/</id>
  <author>
    <name><![CDATA[Rahul Nath]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Exclude Certain Scripts From Transaction When Using DbUp]]></title>
    <link href="http://rahulpnath.com/blog/exclude-certain-scripts-from-transaction-when-using-dbup/"/>
    <updated>2018-12-17T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/exclude-certain-scripts-from-transaction-when-using-dbup</id>
    <content type="html"><![CDATA[<p>Recently I had written about <a href="https://rahulpnath.com/blog/setting-up-dbup-in-azure-pipelines/">Setting Up DbUP in Azure Pipelines</a> at one of my clients. We had all our scripts run under <a href="https://dbup.readthedocs.io/en/latest/more-info/transactions/">Transaction Per Script</a> mode and was all working fine until we had to deploy some SQL scripts that cannot be run under a transaction. So now I have a bunch of SQL script files that can be run under a transaction and some (like the ones below - <a href="https://azure.microsoft.com/en-au/blog/full-text-search-is-now-available-for-preview-in-azure-sql-database/">Full-Text Search</a>) that cannot be run under a transaction. By default, if you run this using DbUp under a transaction you get the error message<span class="text-danger">
CREATE FULLTEXT CATALOG statement cannot be used inside a user transaction </span> and this is an <a href="https://github.com/DbUp/DbUp/issues/207">existing issue</a>.</p>

<pre><code class="sql Full Text Search Script">CREATE FULLTEXT CATALOG MyCatalog
GO

CREATE FULLTEXT INDEX 
ON  [dbo].[Products] ([Description])
KEY INDEX [PK_Products] ON MyCatalog
WITH CHANGE_TRACKING AUTO
GO
</code></pre>

<p>One option would be to turn off transaction all together using <em>builder.WithoutTransaction()</em> (default transaction setting) and everything would work as usual. But in case you want each of your scripts to be run under a transaction you can choose either of the options below.</p>

<h3>Using Pre-Processors to Modify Script Before Execution</h3>

<p><a href="https://dbup.readthedocs.io/en/latest/more-info/preprocessors/">Script Pre-Processors</a> are an extensibility hook into DbUp and allows you to modify a script before it gets executed. So we can wrap each SQL script with a transaction before it gets executed. In this case, you have to configure your builder to run WithoutTransaction and modify each script file before execution and explicitly wrap with a transaction if required. Writing a custom pre-processor is quickly done by implementing the IScriptPreprocessor interface, and you get the contents of the script file to modify. In this case, all I do is check whether the text contains &lsquo;CREATE FULLTEXT&rsquo; and wrap with a transaction if it does not. You could use file-name conventions or any other rules of your choice to perform the check and conditionally wrap with a transaction.</p>

<pre><code class="csharp Conditionally Apply Transaction">public class ConditionallyApplyTransactionPreprocessor : IScriptPreprocessor
{
    public string Process(string contents)
    {
        if (!contents.Contains("CREATE FULLTEXT", StringComparison.InvariantCultureIgnoreCase))
        {
            var modified =
                $@"
BEGIN TRANSACTION   
BEGIN TRY
           {contents}
    COMMIT;
END TRY
BEGIN CATCH
    ROLLBACK;
    THROW;
END CATCH";

            return modified;
        }
        else
            return contents;
    }
}
</code></pre>

<h3>Using Multiple UpgradeEngine to Deploy Scripts</h3>

<p>If you are not particularly fine with tweaking the pre-processing step and want to use the default implementations of DbUp and still achieve keep transactions for you scripts where possible, you can use multiple upgraders to perform the job for you. Iterate over all your script files and then partition them into batches of files that need to be run under a transaction and those that can&rsquo;t be run under a transaction. As shown in the image below you will end up with multiple batches with alternating sets of transaction/non-transaction set of scripts. When performing the upgrade over a batch, set the <em>WithTransactionPerScript</em> on the builder conditionally. If any of the batches fail, you can terminate the database upgrade.</p>

<p><img src="/images/dbup_batches.png" alt="Script file batches" class="center" /></p>

<pre><code class="csharp Execute all batches (Might not be production ready)">{
    Func&lt;string,bool&gt; canRunUnderTransaction = (fileName) =&gt; !fileName.Contains("FullText");
    Func&lt;List&lt;string&gt;, string, bool&gt; belongsToCurrentBatch = (batch, file) =&gt;
        batch != null &amp;&amp;
        canRunUnderTransaction(batch.First()) == canRunUnderTransaction(file);

    var batches = allScriptFiles.Aggregate
        (new List&lt;List&lt;string&gt;&gt;(), (current, next) =&gt;
            {
                if (belongsToCurrentBatch(current.LastOrDefault(),next))
                    current.Last().Add(next);
                else
                    current.Add(new List&lt;string&gt;() { next });

                return current;
            });

    foreach (var batch in batches)
    {
        var includeTransaction = !batch.Any(canRunUnderTransaction);

        var result = PerformUpgrade(batch.ToSqlScriptArray(), includeTransaction);

        if (!result.Successful)
        {
            Console.ForegroundColor = ConsoleColor.Red;
            Console.WriteLine(result.Error);
            Console.ResetColor();
            return -1;
        }
    }

    Console.ForegroundColor = ConsoleColor.Green;
    Console.WriteLine("Success!");
    Console.ResetColor();
    return 0;
}

private static DatabaseUpgradeResult PerformUpgrade(
    SqlScript[] scripts,
    bool includeTransaction)
{
    var builder = DeployChanges.To
        .SqlDatabase(connectionString)
        .WithScripts(scripts)
        .LogToConsole();

    if (includeTransaction)
        builder = builder.WithTransactionPerScript();

      var upgrader = builder.Build();

    var result = upgrader.PerformUpgrade();

    return result;
}
</code></pre>

<p>Keeping all your scripts in a single place and automating it through the build-release pipeline is <a href="https://rahulpnath.com/blog/working-effectively-under-constraints/">something you need to strive for</a>. Hope this helps you to continue using DbUp even if you want to execute scripts that are a mix of transactional and non-transactional.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up DbUp in Azure Pipelines]]></title>
    <link href="http://rahulpnath.com/blog/setting-up-dbup-in-azure-pipelines/"/>
    <updated>2018-12-03T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/setting-up-dbup-in-azure-pipelines</id>
    <content type="html"><![CDATA[<p><a href="https://azure.microsoft.com/en-au/services/devops/pipelines/">Azure Pipelines</a> is part of the <a href="https://azure.microsoft.com/en-au/services/devops/">Azure Devops</a> offerings which enables you to continuously build test and deploy to any platform and cloud environments. It&rsquo;s been a while since this has been out and it&rsquo;s only recently that I have got a chance to play around with it at one of my clients. We use <a href="https://dbup.readthedocs.io/en/latest/">DBUp</a>, a .Net library to deploy schema changes to our SQL Server database. <em>It tracks which SQL scripts have been run already, and runs the change scripts that are needed to get your database up to date.</em></p>

<p>Setting up DbUp is very easy, and you can use the script straight from the <a href="https://dbup.readthedocs.io/en/latest/">docs</a> to get started. If you are using .Net core console application VS template to setup DbUp make sure to modify the return type of the main function to use int and to return the appropriate application exit codes (as from the script in the doc.) I made the mistake of removing the return statements, only to later realize that build scripts were successfully passing even though the DbUp scripts were failing.</p>

<div class="alert alert-warning">
If you are using the .Net Core console application VS template (like I did) make sure you modify the return type of the main function in Program.cs to int. 
</div>


<p>In Azure Pipelines I have the build step publish the build output as a zip artifact. Using this in the release pipeline is a 2 step process</p>

<h4><strong>1 - Extract Zip Package</strong></h4>

<p>Using the <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/utility/extract-files?view=vsts">Extract Files Task</a> extract the zip package from the build artifacts. You can specify a destination folder for the files to be extracted to (as shown below).</p>

<p><img src="/images/dbup_azure_pipelines_extract.jpg" class ="center" alt="Extract package"></p>

<h4><strong>2 - Execute DbUp Package</strong></h4>

<p>With the package extracted out into a folder, we can now execute the console application (using the <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-run?tabs=netcore21#description">dotnet command line</a>) by passing in the connection string as a command line argument.</p>

<p><img src="/images/dbup_azure_pipelines_execute.jpg" class ="center" alt="Execute package"></p>

<p>You now have your database deployments automated through the Azure Pipelines.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing Your Postman API Specs]]></title>
    <link href="http://rahulpnath.com/blog/manage-your-postman-api-specs/"/>
    <updated>2018-07-02T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/manage-your-postman-api-specs</id>
    <content type="html"><![CDATA[<p>In the previous post, we explored how to use <a href="Automated%20API%20Testing%20Using%20Postman%20Collection%20Runner">Postman for testing API endpoints</a>. Postman is an excellent tool to manage API specs as well, so that you can try API requests individually to see how things are working. It also acts as documentation for all your API endpoints and serves as a good starting point for someone new to the team. When it comes to managing the API specs for your application, there are a few options that we have and let&rsquo;s explore what they are.</p>

<h3>Organizing API Specs</h3>

<p>Postman supports the concept of <a href="https://www.getpostman.com/docs/v6/postman/collections/creating_collections">Collections</a>, which are nothing but a Folder to group of saved API requests/Specs. Collections support nesting which means you can add Folders within a collection to further group them. As you can see below the <em>MyApplication</em> and <em>Postman Echo</em> are collections, and there are subfolders inside them which in turn contains API requests. The multi-level hierarchy helps you to organize your requests the way you want to.</p>

<p><img src="/images/postman_collections.png" alt="Postman Collections" class ="center"></p>

<h3>Sharing API Specs</h3>

<p>Any Collection that you create in Postman is automatically synced to Postman Cloud if you are logged in with an account. It allows you to share collections through a link. With <a href="https://www.getpostman.com/pricing">paid version of Postman</a> you get to create <a href="https://www.getpostman.com/workspaces">team workspaces</a>, which means a team can collaborate on the shared versions. It allows easy sharing of specs across your team and manages them in a centralized place.</p>

<p>However, if you are not logged in or don&rsquo;t have a paid version of Postman, you can maintain the specs along with your Source Code. Postman allows you to <a href="https://www.getpostman.com/docs/v6/postman/collections/sharing_collections#sharing-as-a-file">export Collections and share specs as a JSON file</a>. You can then check this file into your source code repository. Other team members can Import the exported file to get the latest specs. The only disadvantage with this is that you need to make sure to export/import every time you/other team members make a change to the JSON file. However, I have seen this approach work well in teams and one way we made sure that the JSON file was up to date is to have to update the API spec as a Work Item and which required to be <a href="https://rahulpnath.com/blog/code-review/">peer reviewed</a>(through Pull Requests)</p>

<h3>Managing Environments</h3>

<p>Typically any application/API would be deployed to multiple environments (like localhost, Development, Testing, Production, etc.) and you would want to switch between these environments to test your API endpoints seamlessly. Postman makes this easy by using the <a href="https://www.getpostman.com/docs/v6/postman/environments_and_globals/manage_environments">Environment Feature</a>.</p>

<p><img src="/images/postman_environment.png" alt="Postman Environment" class="center" /></p>

<p>Again as with Collections, Environments are also synced to Postman Cloud when you are logged in. It makes all your environments available to all your team seamlessly. However, if you are not logged in you can again export the environments as a JSON file and then share that out of band (in a secure manner as this might have sensitive information like tokens, keys, etc.) with your team.</p>

<h3>Publishing API Specs</h3>

<p>Postman allows you to <a href="https://www.getpostman.com/docs/v6/postman/api_documentation/publishing_public_docs">publish API specs</a> (even to a custom URL), which can act like your API Documentation. You can publish it per environments and also easily execute them. Publishing is available only if you log in to an account as it requires the API Specs and environment details in the first place.</p>

<p><img src="/images/postman_published.png" alt="Postman Published" class="center" /></p>

<h3>Security Considerations</h3>

<p>When using the sync feature of Postman (logged in to the application with Postman account), it is <a href="https://www.getpostman.com/docs/v6/postman_for_publishers/run_button/security">recommended</a> that you do not have any sensitive information (like passwords/tokens) as part of the API request spec/Collection. These should be extracted out as Environment variables and stored as part of the appropriate environment.</p>

<p>If you are logged in, all the data that you add to it is automatically synced, which means it will be living in Postman&rsquo;s cloud server. This might not be a desirable option for every company but looks like there is no option to turn sync off at the Collection level. The only way to not sync collections is to not log into an account in Postman.</p>

<div class="alert alert-warning">
    <i>
    If you are logged into Postman then any collection that you create is automatically synced to Postman server. Only way to <a href="https://support.getpostman.com/hc/en-us/articles/203492852-How-do-I-disable-Sync-">prevent sync</a> is not to log in
    </i>
</div>


<p>We have seen the options by which you can share API collections and environments amongst your team even if you are logged in. However, one thing to be aware of is if any of your team members are logged into Postman and imports a collection shared via Repository/out of band methods, it will be synced to Postman server. So at the organization/team level, you would need ways to prevent this from happening if it is essential for you. Best is to have your API&rsquo;s designed in such a way that you do not have to expose such sensitive information, which anyways is a better practice.</p>

<p>Hope this allows to manage your API specs better!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NDC Security 2018 - Overview and Key Takeaways]]></title>
    <link href="http://rahulpnath.com/blog/ndc-security-2018-overview-and-key-takeaways/"/>
    <updated>2018-05-28T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/ndc-security-2018-overview-and-key-takeaways</id>
    <content type="html"><![CDATA[<p>While in Sydney I was lucky enough to have attended the <a href="https://rahulpnath.com/blog/ndc-sydney/">first</a> and <a href="https://rahulpnath.com/blog/ndc-sydney-2017/">second</a> NDC Conferences. After moving up to Brisbane, did not think I could attend one of these soon. However, then comes a nice shorter version of NDC specific to Security - <a href="https://ndcsecurity.com.au/">NDC Security</a>. As the name suggests, this conference is particular to security-related topics with a 2-day workshop and 1-day conference, as was held in Gold Coast, Queensland.</p>

<h3>The Workshop</h3>

<p><a href="https://www.troyhunt.com/">Troy Hunt</a> and <a href="https://scotthelme.co.uk/">Scott Helme</a> ran <a href="https://ndcsecurity.com.au/workshops/">two workshops</a> and I attended <a href="https://ndcsecurity.com.au/workshop/hack-yourself-first-how-to-go-on-the-cyber-offence/">Hack Yourself First</a> by Troy. The workshop covers a wide range of topics and is perfect for anyone who is into web development. The best thing is that you only need to have a browser and <a href="https://www.telerik.com/download/fiddler">Fiddler</a>/<a href="https://www.charlesproxy.com/download/">Charles Proxy</a> (depending on whether you are on Windows or Mac land). One of the interesting thing about the workshop is that it puts you first into the hackers perspective and forces you to exploit existing vulnerabilities in the <a href="http://hackyourselffirst.troyhunt.com/">sample site</a> designed specifically for this. Once you can do this, we then look at ways of protecting ourselves against such exploits and other mechanisms involved.</p>

<p><img src="/images/ndc_security_hyf.jpg" class="center" alt="Hack yourself first, Troy Hunt"></p>

<p>The workshop highlights how easy it is to find and exploit vulnerabilities in applications. Some tools detect vulnerabilities and exploit them for you if you input a few details to them. You necessarily need not know the vulnerabilities itself or how exactly to exploit them. Such tools make it easy for people to use them on any website that is out there on the web. Combined with the power of search engines it makes it quite easy to make your site vulnerabilities to be easily discoverable.</p>

<h3>The Conference</h3>

<p>There were <a href="https://ndcsecurity.com.au/agenda/">six talks</a> in total and below are the ones that I found interesting.</p>

<ul>
<li><a href="https://ndcsecurity.com.au/talk/csp-xxp-sts-pkp-caa-etc-omg-wtf-bbq/">Scott Helme Talk: CSP XXP STS PKP CAA ETC OMG WTF BBQâ€¦</a></li>
<li><a href="https://ndcsecurity.com.au/talk/dependable-dependencies/">Talk: Dependable Dependencies</a></li>
<li><a href="https://ndcsecurity.com.au/talk/everything-is-cyber-broken/">Everything is Cyber-broken</a></li>
</ul>


<p><img src="/images/ndc_security_conference.jpg" class="center" alt="NDC Securtiy, 2018 - Conference"></p>

<p>The whole web is on a journey towards making it more secure. So it is an excellent time to <a href="https://rahulpnath.com/blog/https-for-free-and-why-you-should-care/">move on to HTTPS</a> if you are not already. Even after enabling HTTPS, it is a good idea to make sure you have got all the appropriate <a href="https://securityheaders.com/">security headers</a> set. Making sure that the libraries that you depend on are patched and updated is equally essential.
There are incidents of massive data breaches because of vulnerabilities in third-party libraries and not keeping them updated.</p>

<blockquote><p><em>Functionality need not be the only reason to upgrade third-party libraries. There might be security vulnerabilities that are getting patched which is an equally good reason to update dependent packages</em></p></blockquote>

<p>The harder thing is to keep track of the vulnerabilities that are getting reported and always checking back with your application&rsquo;s dependencies. There is a wide range of tools that help make this easy and seamlessly integrate within the development workflow. It can be included as early as when a developer intends to include a library into the source code, or in the build pipeline or even for sites that are up and running. The earlier such issues get detected in the software development lifecycle, the less costly and impact it has on time and cost.</p>

<h4><strong>Tools</strong></h4>

<ul>
<li><a href="https://sonarwhal.com/scanner/">Sonarwhal</a></li>
<li><a href="https://snyk.io/">Snyk</a></li>
<li><a href="https://retirejs.github.io/retire.js/">Retire.js</a></li>
<li><a href="https://www.netsparker.com/">Netsparker</a></li>
<li><a href="https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project">OWASP Zed Attack Proxy Project</a></li>
<li><a href="https://www.troyhunt.com/troys-ultimate-list-of-security-links/">Ultimate List of Security Links</a></li>
</ul>


<p>The conference ended with a good discussion between Troy and Scott on how everything is Cyber broken. It touches upon the value of Extended Validation (EV) Certificate and how CA&rsquo;s are trying to push for them while browsers are more and more going away from them. It also touches on various proponents of HTTP and the wrong messages that are getting spread to a broader audience and also about <a href="https://scotthelme.co.uk/revocation-is-broken/">certificate revocations</a> and a lot more. It was a fun discussion and a great end to the three-day event.</p>

<h3>Location and Food</h3>

<p>NDC Security was held at <a href="https://www.qthotelsandresorts.com/gold-coast/">QT Gold Coast, Queensland</a> and well organized. Coffee and drinks were available all throughout the day with a barista on the last day (which was cool). Food was served at start, breaks, and lunch and was good. The conference rooms were great and spacious and had reasonable good internet. Did not face much connectivity issues and everything ran smoothly.</p>

<p><img src="/images/ndc_security_food_location.jpg" class="center" alt="NDC Securtiy, 2018 - Food and Location"></p>

<p>One of the things I first did after coming from the conference was to <a href="https://rahulpnath.com/blog/https-for-free-and-why-you-should-care/">move this blog over to HTTPS</a>. I had been procrastinating long on this, but there were enough reasons to make a move now. Also, there are a bunch of things that catch my eye at client places and other web sites that I visit often. Attending the conference and workshop has been a great value add and recommend to anyone if you have a chance to attend that. For the others, most of the content is available in <a href="https://www.pluralsight.com/">Pluralsight</a>.</p>

<p><em>PS: Special thanks to <a href="https://rahulpnath.com/blog/finding-a-job-abroad/">Readify</a> for sending me to this conference and also providing a &lsquo;paid vacation (accommodation)&rsquo; in Gold Coast. It was a nice three-day break for my wife and son also.</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up Build and Deploy Pipeline for a .NET Core Console Application]]></title>
    <link href="http://rahulpnath.com/blog/build-and-deploy-a-net-core-console-application/"/>
    <updated>2018-01-03T00:00:00+00:00</updated>
    <id>http://rahulpnath.com/blog/build-and-deploy-a-net-core-console-application</id>
    <content type="html"><![CDATA[<p>I was given a console application written in .NET Core 2.0 and asked to set up a continuous deployment pipeline using <a href="https://www.jetbrains.com/teamcity/">TeamCity</a> and <a href="https://octopus.com/">Octopus Deploy</a>. I struggled a bit with some parts, so thought it&rsquo;s worth putting together a post on how I went about it. If you have a better or different way of doing things, please shout out in the comments below.</p>

<p>At the end of this post, we will have a console application that is automatically deployed to a server and running, anytime a change is pushed to the associated source control repository.</p>

<h3>Setting Up TeamCity</h3>

<p>Create a <a href="https://confluence.jetbrains.com/display/TCD10/Creating+and+Editing+Projects">New Project</a> and add a <a href="https://confluence.jetbrains.com/display/TCD10/Creating+and+Editing+Build+Configurations">new build configuration</a> just like you would for any other project. Since the application is in .NET Core, install the <a href="https://github.com/JetBrains/teamcity-dotnet-plugin">.NET CLI plugin</a> on the TeamCity server.</p>

<p><img src="/images/net_core_teamcity_build_steps.png" alt="Build Steps to build .Net Core"></p>

<p>The first three build steps use the .NET CLI to <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-restore?tabs=netcore2x">Restore</a>, <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-build?tabs=netcore2x">Build</a> and <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-publish?tabs=netcore2x">Publish</a> the application. Thee three steps restore the dependencies of the project, builds it and publishes all the relevant DLL&rsquo;s into the publish folder.</p>

<p>The published application now needs to be packaged for deployment. In my case, deployments are managed using Octopus Deploy. For .NET projects, the preferred way of packaging for Octopus is using <a href="https://octopus.com/docs/packaging-applications/creating-packages/nuget-packages/using-octopack">Octopack</a>. However, OctoPack does not support .NET Core projects. The recommendation is to either use <a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-pack?tabs=netcore2x">dotnet pack</a> or <a href="https://octopus.com/docs/packaging-applications/creating-packages/nuget-packages/using-octo.exe">Octo.exe pack</a>. Using the latter I have set up a Command Line build step to pack the contents of the published folder into a zip (.nupkg) file.</p>

<pre><code class="bash">octo pack --id ApplicationName --version %build.number% --basePath published-app 
</code></pre>

<p>The NuGet package is published to the NuGet server used by Octopus. Using the <a href="https://octopus.com/docs/api-and-integration/teamcity">Octopus Deploy: Create Release</a> build step, a new release is triggered in Octopus Deploy.</p>

<h3>Setting Up Octopus Deploy</h3>

<p>Create a <a href="https://octopus.com/docs/deployment-process/projects">new project</a> in Octopus Deploy to manage deployments. Under the Process tab, I have two <a href="https://octopus.com/docs/deployment-process/steps">steps</a> - one to deploy the Package and another to start the application.</p>

<p><img src="/images/net_core_octopus_deploy_process.png" alt="Octopus Deploy Process Steps"></p>

<p>For the Deploy Package step I have enabled Custom Deployment Scripts and <a href="https://octopus.com/docs/deploying-applications/deploying-asp.net-core-web-applications/json-configuration-variables-feature">JSON Configuration variables</a>. Under the pre-deployment script, I stop any existing .NET applications. If multiple .NET applications are running on the box, select your application explicitly.</p>

<pre><code class="powershell Pre Deployment Script">Stop-Process -Name dotnet -Force -ErrorAction SilentlyContinue
</code></pre>

<p>Once the package is deployed, the custom script starts up the application.</p>

<pre><code class="powershell Run App">cd C:\DeploymentFolder
Start-Process dotnet .\ApplicationName.dll
</code></pre>

<p>With all that set, any time a change is pushed into the source control repository, TeamCity picks that up, build and triggers a deployment to the configured environments in Octopus Deploy. Hope this helps!</p>
]]></content>
  </entry>
  
</feed>
